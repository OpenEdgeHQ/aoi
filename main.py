# main.py
import asyncio
import json
from datetime import datetime
from typing import Dict, Any, Optional, List
import traceback
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å¯¼å…¥æ—¥å¿—
from utils.logger_config import AgentLogger, setup_logging, FileLogHandler

# å¯¼å…¥tokené™åˆ¶å·¥å…·
from utils.token_limiter import truncate_context, get_token_limiter

# å¯¼å…¥ç¯å¢ƒå®¢æˆ·ç«¯
from environment.aiopslab_client import EnvironmentClient

# å¯¼å…¥æ™ºèƒ½ä½“
from agents.observer_agent import ObserverAgent
from agents.probe_agent import ProbeAgent
from agents.executor_agent import ExecutorAgent
from agents.compressor_agent import CompressorAgent

# å¯¼å…¥Memoryç›¸å…³
from memory.memory_manager import MemoryManager
from memory.memory_item import (
    AgentType, SubTaskItem, RawContextItem,
    CompressedContextItem, TaskStatus, MemoryType
)

# å¯¼å…¥AWorldé…ç½®
from aworld.config.conf import AgentConfig


class AIOPlatform:
    """AIè¿ç»´å¹³å°ä¸»ç±»"""

    def __init__(self,
                 llm_config: AgentConfig,
                 env_client: Optional[EnvironmentClient] = None,
                 max_iterations: int = 6,
                 max_context_tokens: int = 25000,
                 max_output_tokens: int = 8000,
                 debug_no_submit: bool = False):
        """
        åˆå§‹åŒ–AIè¿ç»´å¹³å°

        Args:
            llm_config: LLMé…ç½®
            env_client: ç¯å¢ƒå®¢æˆ·ç«¯
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            max_context_tokens: æœ€å¤§ä¸Šä¸‹æ–‡tokenæ•°ï¼ˆé˜²æ­¢è¶…é•¿ï¼‰
            max_output_tokens: æœ€å¤§è¾“å‡ºtokenæ•°
            debug_no_submit: Debugæ¨¡å¼ï¼šä¸çœŸæ­£æäº¤ï¼Œåªæ‰“å°æäº¤å‘½ä»¤ï¼ˆsessionä¸ä¼šå…³é—­ï¼‰
        """
        # åˆå§‹åŒ–æ—¥å¿—
        setup_logging()
        self.logger = AgentLogger("PLATFORM")

        # é…ç½®
        self.llm_config = llm_config
        self.max_iterations = max_iterations
        self.max_context_tokens = max_context_tokens
        self.max_output_tokens = max_output_tokens
        self.debug_no_submit = debug_no_submit

        # ç¯å¢ƒå®¢æˆ·ç«¯
        self.env_client = env_client
        self.session_id = None
        self.task_info = {}
        self.submit_format = {}

        # åˆå§‹åŒ–Memoryç®¡ç†å™¨
        self.memory_manager = MemoryManager(
            max_raw_items=100000,
            max_compressed_items=10000,
            max_task_items=2000,
        )

        # æ™ºèƒ½ä½“å°†åœ¨runæ—¶åˆå§‹åŒ–
        self.observer = None
        self.probe = None
        self.executor = None
        self.compressor = None

        # æ‰§è¡ŒçŠ¶æ€
        self.current_iteration = 0
        self.execution_history = []
        
        # å­˜å‚¨problem_id
        self.problem_id = None

        # ç»“æœæ”¶é›†
        self.execution_results = {
            "observer_outputs": [],
            "probe_results": [],
            "executor_results": [],
            "compressor_outputs": [],
            "iterations": []
        }
        
        # è¯„ä¼°ç»“æœï¼ˆä»AIOpsLabè¿”å›ï¼‰
        self.evaluation_results = {}

    def _initialize_agents(self, task_info: Dict[str, Any]):
        """åˆå§‹åŒ–æ‰€æœ‰æ™ºèƒ½ä½“"""
        self.task_info = task_info

        # è·å–submitæ ¼å¼
        self.submit_format = self.env_client.get_submit_format() if self.env_client else {}

        # æå–ä»»åŠ¡ä¿¡æ¯
        task_description = task_info.get('task_description', '')
        available_actions = task_info.get('available_actions', {})
        api_instruction = task_info.get('instructions', '')
        print("=" * 60)
        print(f"task_description: {task_description}")
        print("=" * 60)
        # åˆ›å»ºObserver - å®ƒä¼šè‡ªåŠ¨åˆå§‹åŒ–å­ä»»åŠ¡é˜Ÿåˆ—ï¼Œå¹¶ä»problem_idä¸­æå–ä»»åŠ¡ç±»å‹
        self.observer = ObserverAgent(
            llm_config=self.llm_config,
            memory_manager=self.memory_manager,
            max_iterations=self.max_iterations,
            task_description=task_description,
            available_actions=available_actions,
            api_instruction=api_instruction,
            submit_format=self.submit_format,
            problem_id=self.problem_id  # ä¼ é€’problem_idç”¨äºæå–ä»»åŠ¡ç±»å‹
        )

        # åˆ›å»ºProbe
        self.probe = ProbeAgent(
            llm_config=self.llm_config,
            memory_manager=self.memory_manager,
            max_iterations=3,
            task_description=task_description,
            available_actions=available_actions,
            api_instruction=api_instruction
        )

        # åˆ›å»ºExecutor
        self.executor = ExecutorAgent(
            llm_config=self.llm_config,
            memory_manager=self.memory_manager,
            probe_agent=self.probe,
            max_iterations=1,
            use_probe=True,
            task_description=task_description,
            available_actions=available_actions,
            api_instruction=api_instruction
        )

        # åˆ›å»ºCompressor
        self.compressor = CompressorAgent(
            llm_config=self.llm_config,
            memory_manager=self.memory_manager,
            max_output_tokens=self.max_output_tokens,
            max_context_tokens=self.max_context_tokens
        )

        self.logger.info("âœ… All agents initialized successfully")

        # æ˜¾ç¤ºåˆå§‹å­ä»»åŠ¡é˜Ÿåˆ—
        self.logger.info(f"\nğŸ“‹ Initial Task Queue:")
        for i, task in enumerate(self.observer.task_queue, 1):
            submit_marker = " [SUBMIT]" if task.is_submit_task else ""
            self.logger.info(
                f"  {i}. {task.task_name} "
                f"({task.target_agent.value if task.target_agent else 'unknown'})"
                f"{submit_marker}"
            )

    def execute_action(self, command: str) -> str:
        """æ‰§è¡Œå‘½ä»¤"""
        if self.env_client:
            result = self.env_client.execute_action(command)
            if isinstance(result, dict):
                # å¦‚æœæ˜¯æäº¤åŠ¨ä½œï¼Œä¿å­˜è¯„ä¼°ç»“æœ
                if result.get('is_submission') and result.get('evaluation'):
                    self.evaluation_results = result['evaluation']
                    self.logger.info(f"\nğŸ“Š Evaluation Results: {json.dumps(self.evaluation_results, indent=2)}")
                
                if result.get('error', False):
                    return f"Error: {result.get('result', str(result))}"
                return result.get('result', str(result))
            return str(result)
        else:
            return f"[Simulated execution] {command}"

    async def run_iteration(self, iteration: int) -> Dict[str, Any]:
        """è¿è¡Œå•æ¬¡è¿­ä»£ - åŸºäºå­ä»»åŠ¡é˜Ÿåˆ—"""
        self.logger.info(f"\n{'=' * 80}")
        self.logger.info(f"ğŸ“ ITERATION {iteration}/{self.max_iterations}")
        self.logger.info(f"{'=' * 80}")

        iteration_data = {
            "iteration": iteration,
            "timestamp": datetime.now().isoformat(),
            "actions": []
        }

        try:
            # è·å–å½“å‰å­ä»»åŠ¡
            current_subtask = self.observer.get_current_subtask()

            if current_subtask:
                self.logger.info(f"\nğŸ“‹ Current Subtask: {current_subtask.task_name}")
                self.logger.info(
                    f"   Target Agent: {current_subtask.target_agent.value if current_subtask.target_agent else 'Unknown'}")
                self.logger.info(f"   Objective: {current_subtask.task_objective}")

                # å¦‚æœæ˜¯æäº¤ä»»åŠ¡
                if current_subtask.is_submit_task:
                    self.logger.success(f"\nğŸ“® Executing SUBMISSION task!")

                    # æäº¤ä»»åŠ¡ä¹Ÿéµå¾ªç›¸åŒåŸåˆ™ï¼šä½¿ç”¨ previous_iteration_context
                    decision = await self.observer.analyze_and_decide(
                        compressed_context="",  # ä¸éœ€è¦ä¼ å…¥ï¼Œä½¿ç”¨ previous_iteration_context
                        iteration=iteration
                    )

                    # ä½¿ç”¨Observerå†³å®šçš„æäº¤å‘½ä»¤
                    submission_command = decision.get("submission_command", "submit()")

                    # æ‰§è¡Œæäº¤
                    if self.env_client:
                        if self.debug_no_submit:
                            # Debugæ¨¡å¼ï¼šåªæ‰“å°ä¸æäº¤
                            self.logger.warning(f"ğŸ” [DEBUG MODE] Would submit: {submission_command}")
                            self.logger.warning(f"ğŸ” [DEBUG MODE] Submission skipped - session remains open")
                            submit_result = {"status": "DEBUG_SKIP", "message": "Submission skipped in debug mode"}
                        else:
                            # æ­£å¸¸æ¨¡å¼ï¼šçœŸæ­£æäº¤
                            submit_result = self.execute_action(submission_command)
                            self.logger.info(f"[SUBMIT] Command: {submission_command}")
                            self.logger.info(f"[SUBMIT] Result: {submit_result}")

                        # è®°å½•æäº¤ç»“æœ
                        iteration_data["actions"].append({
                            "type": "submit",
                            "command": submission_command,
                            "result": str(submit_result)
                        })

                        # æ ‡è®°ä»»åŠ¡å®Œæˆ
                        if current_subtask:
                            current_subtask.complete_execution(True, f"Submitted: {submission_command}")
                            self.memory_manager.update_item(current_subtask, AgentType.OBSERVER)

                        # Debugæ¨¡å¼ï¼šä¸ç»“æŸï¼Œç»§ç»­æ‰§è¡Œ
                        if self.debug_no_submit:
                            self.logger.warning(f"ğŸ” [DEBUG MODE] Continuing execution (normally would have ended here)")
                            self.execution_results["iterations"].append(iteration_data)
                            # ä¸è¿”å›ï¼Œè®©å‡½æ•°ç»§ç»­æ‰§è¡Œåé¢çš„æ­£å¸¸æµç¨‹
                        else:
                            # æ­£å¸¸æ¨¡å¼ï¼šæ›´å‡†ç¡®åœ°æ£€æŸ¥æ˜¯å¦è§£å†³
                            if isinstance(submit_result, dict):
                                result_str = str(submit_result.get('result', submit_result))
                            else:
                                result_str = str(submit_result)

                            # æ£€æŸ¥å¤šç§æˆåŠŸæ ‡å¿—
                            if any(indicator in result_str for indicator in
                                   ["VALID_SUBMISSION", "VALID", "solved", "complete"]) or \
                                    self.env_client.is_problem_solved():
                                self.logger.success(f"\nâœ… Problem SOLVED!")
                                self.execution_results["iterations"].append(iteration_data)
                                return {
                                    "status": "completed",
                                    "solution": submission_command,
                                    "iterations": iteration,
                                    "result": "VALID"
                                }
                            elif "INVALID_SUBMISSION" in result_str:
                                self.logger.warning(f"\nâš ï¸ Invalid submission - solution does not meet requirements")
                                self.execution_results["iterations"].append(iteration_data)
                                return {
                                    "status": "completed",
                                    "solution": submission_command,
                                    "iterations": iteration,
                                    "result": "INVALID"
                                }
                            else:
                                self.logger.info(f"\nğŸ“¨ Submission received, status: {result_str[:100]}")
                                self.execution_results["iterations"].append(iteration_data)
                                return {
                                    "status": "completed",
                                    "solution": submission_command,
                                    "iterations": iteration,
                                    "result": "SUBMITTED"
                                }
            
            # å¦‚æœæ˜¯debugæ¨¡å¼ä¸”å·²ç»"æäº¤"è¿‡ï¼Œè·³è¿‡åç»­å¤„ç†
            if self.debug_no_submit and current_subtask and current_subtask.is_submit_task:
                return iteration_data

            # 1. Observer åˆ†æå¹¶ç”Ÿæˆå…·ä½“æŒ‡ä»¤
            #    è¾“å…¥ï¼šä¸Šä¸€è½®çš„ compressed contextï¼ˆåœ¨ observer.previous_iteration_context ä¸­ï¼‰
            #    åŒæ—¶ç”Ÿæˆä¸Šä¸€è½®çš„æ€»ç»“
            self.logger.info(f"\n[OBSERVER] Analyzing and generating instructions")

            decision = await self.observer.analyze_and_decide(
                compressed_context="",  # ä¸éœ€è¦ä¼ å…¥ï¼ŒObserver ä¼šä» previous_iteration_context è·å–
                iteration=iteration
            )

            # è®°å½•Observerå†³ç­–
            self.execution_results["observer_outputs"].append({
                "iteration": iteration,
                "decision": decision,
                "timestamp": datetime.now().isoformat()
            })

            # è·å–å½“å‰å­ä»»åŠ¡ï¼ˆä»å†³ç­–ä¸­ï¼‰
            current_subtask = decision.get('current_subtask')

            if not current_subtask:
                self.logger.error("No subtask available!")
                self.execution_results["iterations"].append(iteration_data)
                return {"status": "error", "error": "No subtask available"}

            # è¾“å‡ºå†³ç­–ä¿¡æ¯
            self.logger.info(f"[OBSERVER] Decision: Activate {decision.get('next_agent', 'Unknown').upper()}")
            self.logger.info(f"[OBSERVER] Subtask: {current_subtask.task_name}")

            # æ·»åŠ åˆ°æ‰§è¡Œå†å²
            self.observer.add_execution_result(
                agent_type="observer",
                action=f"Subtask: {current_subtask.task_name}",
                result=decision.get('reasoning', '')[:200],
                status="success"
            )

            # 3. æ ¹æ®å­ä»»åŠ¡ç›®æ ‡æ‰§è¡Œç›¸åº”æ™ºèƒ½ä½“
            next_agent = decision.get("next_agent", "")
            task_instruction = decision.get("instruction", "")

            if next_agent == "complete" or decision.get("ready_to_submit", False):
                # æäº¤ä»»åŠ¡
                self.logger.success(f"\nâœ… Executing submission!")

                # ä½¿ç”¨Observerç”Ÿæˆçš„æäº¤å‘½ä»¤ï¼ˆè€Œä¸æ˜¯ç¡¬ç¼–ç ï¼‰
                submission_command = decision.get('submission_command', 'submit()')

                if self.env_client:
                    if self.debug_no_submit:
                        # Debugæ¨¡å¼ï¼šåªæ‰“å°ä¸æäº¤
                        self.logger.warning(f"ğŸ” [DEBUG MODE] Would submit: {submission_command}")
                        self.logger.warning(f"ğŸ” [DEBUG MODE] Submission skipped - session remains open")
                        submit_result = {"status": "DEBUG_SKIP", "message": "Submission skipped in debug mode"}
                    else:
                        # æ­£å¸¸æ¨¡å¼ï¼šçœŸæ­£æäº¤
                        submit_result = self.execute_action(submission_command)
                        self.logger.info(f"[SUBMIT] Command: {submission_command}")
                        self.logger.info(f"[SUBMIT] Result: {submit_result}")

                    iteration_data["actions"].append({
                        "type": "submit",
                        "command": submission_command,
                        "result": str(submit_result)
                    })

                    # æ ‡è®°ä»»åŠ¡å®Œæˆ
                    if current_subtask:
                        current_subtask.complete_execution(True, f"Submitted: {submission_command}")
                        self.memory_manager.update_item(current_subtask, AgentType.OBSERVER)

                    # Debugæ¨¡å¼ï¼šä¸ç»“æŸï¼Œè¿”å›iteration_dataä»¥ç»§ç»­ä¸‹ä¸€è½®
                    if self.debug_no_submit:
                        self.logger.warning(f"ğŸ” [DEBUG MODE] Continuing execution (normally would have ended here)")
                        self.execution_results["iterations"].append(iteration_data)
                        return iteration_data  # è¿”å›è€Œä¸æ˜¯continueï¼Œè®©å¤–å±‚å¾ªç¯ç»§ç»­
                    
                    # æ­£å¸¸æ¨¡å¼ï¼šæ£€æŸ¥ç»“æœå¹¶è¿”å›
                    if "VALID" in str(submit_result) or self.env_client.is_problem_solved():
                        self.execution_results["iterations"].append(iteration_data)
                        return {
                            "status": "completed",
                            "solution": submission_command,
                            "iterations": iteration
                        }

                # æ­£å¸¸æ¨¡å¼ï¼šè¿”å›å®ŒæˆçŠ¶æ€
                self.execution_results["iterations"].append(iteration_data)
                return {
                    "status": "completed",
                    "solution": f"Task completed with: {submission_command}",
                    "iterations": iteration
                }


            elif next_agent == "probe":
                # æ‰§è¡Œæ¢æµ‹ä»»åŠ¡
                self.logger.info(f"\n[PROBE] Executing subtask: {current_subtask.task_name}")

                result = await self.probe.probe_run(
                    task_instruction=task_instruction,
                    execute_action=self.execute_action,
                    current_subtask=current_subtask,
                    session_id=self.session_id
                )

                # è®°å½•æ¢æµ‹ç»“æœ
                self.execution_results["probe_results"].append({
                    "iteration": iteration,
                    "subtask": current_subtask.task_name,
                    "result": result,
                    "timestamp": datetime.now().isoformat()
                })

                iteration_data["actions"].append({
                    "type": "probe",
                    "subtask": current_subtask.task_name,
                    "rounds": result.get('total_rounds', 0),
                    "successful_commands": result.get('successful_commands', 0)
                })
                # æ›´æ–°å­ä»»åŠ¡çŠ¶æ€
                if result.get('completed'):
                    current_subtask.complete_execution(True, f"Completed {result['total_rounds']} rounds")
                else:
                    current_subtask.execution_rounds += result.get('total_rounds', 1)

                self.memory_manager.update_item(current_subtask, AgentType.OBSERVER)

                # è¾“å‡ºç»“æœæ‘˜è¦
                self.logger.info(f"[PROBE] Completed {result['total_rounds']} rounds")
                self.logger.info(
                    f"[PROBE] Success: {result['successful_commands']}, Failed: {result['failed_commands']}")

                # æ·»åŠ åˆ°æ‰§è¡Œå†å²
                self.observer.add_execution_result(
                    agent_type="probe",
                    action=f"Probe: {current_subtask.task_name}",
                    result=result.get('probe_history', '')[:500],
                    status="success" if result.get('completed') else "partial"
                )

            elif next_agent == "executor":
                # æ‰§è¡Œä¿®å¤ä»»åŠ¡
                self.logger.info(f"\n[EXECUTOR] Executing subtask: {current_subtask.task_name}")
                
                # è·å–executor_contextï¼ˆå¦‚æœæœ‰ï¼‰
                executor_context = decision.get("executor_context", "")
                if executor_context:
                    self.logger.info(f"[EXECUTOR] Received context from Observer ({len(executor_context)} chars)")

                result = await self.executor.executor_run(
                    task_instruction=task_instruction,
                    execute_action=self.execute_action,
                    current_subtask=current_subtask,
                    session_id=self.session_id,
                    executor_context=executor_context
                )

                # è®°å½•æ‰§è¡Œç»“æœ
                self.execution_results["executor_results"].append({
                    "iteration": iteration,
                    "subtask": current_subtask.task_name,
                    "result": result,
                    "timestamp": datetime.now().isoformat()
                })

                iteration_data["actions"].append({
                    "type": "executor",
                    "subtask": current_subtask.task_name,
                    "rounds": result.get('total_rounds', 0),
                    "successful_commands": result.get('successful_commands', 0)
                })

                # æ›´æ–°å­ä»»åŠ¡çŠ¶æ€
                if result.get('completed'):
                    current_subtask.complete_execution(True, f"Completed {result['total_rounds']} rounds")
                else:
                    current_subtask.execution_rounds += result.get('total_rounds', 1)

                self.memory_manager.update_item(current_subtask, AgentType.OBSERVER)

                # è¾“å‡ºç»“æœæ‘˜è¦
                self.logger.info(f"[EXECUTOR] Completed {result['total_rounds']} rounds")
                self.logger.info(
                    f"[EXECUTOR] Success: {result['successful_commands']}, Failed: {result['failed_commands']}")

                # æ·»åŠ åˆ°æ‰§è¡Œå†å²
                self.observer.add_execution_result(
                    agent_type="executor",
                    action=f"Executor: {current_subtask.task_name}",
                    result=result.get('execution_history', '')[:500],
                    status="success" if result.get('completed') else "partial"
                )

            # Iter n æ‰§è¡Œå®Œæˆåï¼š
            # å‹ç¼©æœ¬è½® (iter n) çš„ RAW_CONTEXTï¼Œä¿å­˜ä¸º compressed context ä¾›ä¸‹ä¸€è½®ä½¿ç”¨
            if iteration < self.max_iterations:  # ä¸éœ€è¦ä¸ºæœ€åä¸€è½®å‹ç¼©
                try:
                    self.logger.info(f"\n[COMPRESSOR] Compressing iteration {iteration} RAW_CONTEXT")
                    compressed_context = await self.compressor.compressor_run(
                        session_id=self.session_id,
                        current_subtask=current_subtask
                    )
                    
                    # Limit token count
                    token_limiter = get_token_limiter(self.llm_config.llm_config.llm_model_name)
                    original_tokens = token_limiter.count_tokens(compressed_context)
                    
                    if original_tokens > self.max_context_tokens:
                        self.logger.warning(
                            f"âš ï¸  Context too long ({original_tokens} tokens), "
                            f"truncating to {self.max_context_tokens} tokens"
                        )

                        compressed_context = truncate_context(
                            compressed_context,
                            self.max_context_tokens,
                            self.llm_config.llm_config.llm_model_name
                        )
                        final_tokens = token_limiter.count_tokens(compressed_context)
                        self.logger.info(f"[COMPRESSOR] After truncation: {final_tokens} tokens")
                    else:
                        self.logger.info(f"[COMPRESSOR] Token count: {original_tokens} tokens (OK)")
                    
                    # ä¿å­˜å½“å‰ iter n çš„å‹ç¼©ä¸Šä¸‹æ–‡ä¾› iter n+1 ä½¿ç”¨
                    self.observer.previous_iteration_context = compressed_context
                    self.logger.info(f"[COMPRESSOR] Saved iteration {iteration} compressed context for next iteration")
                    
                    # è®°å½•å‹ç¼©ç»“æœ
                    self.execution_results["compressor_outputs"].append({
                        "iteration": iteration,
                        "compressed_context": compressed_context[:1000],
                        "size": len(compressed_context)
                    })
                    
                except Exception as e:
                    self.logger.error(f"Failed to compress context: {e}")
            
            # å‰è¿›åˆ°ä¸‹ä¸€ä¸ªä»»åŠ¡
            self.observer.advance_to_next_task()

            self.execution_results["iterations"].append(iteration_data)
            return {"status": "continue"}

        except Exception as e:
            self.logger.error(f"Error in iteration {iteration}: {str(e)}")
            self.logger.error(traceback.format_exc())

            # æ ‡è®°å½“å‰ä»»åŠ¡å¤±è´¥
            current_subtask = self.observer.get_current_subtask()
            if current_subtask:
                current_subtask.mark_failed(str(e))
                self.memory_manager.update_item(current_subtask, AgentType.OBSERVER)
                self.observer.advance_to_next_task()

            self.execution_results["iterations"].append(iteration_data)
            return {"status": "error", "error": str(e)}

    def log_token_usage(self):
        """æ˜¾ç¤ºtokenä½¿ç”¨ç»Ÿè®¡"""
        if not self.observer:
            return
            
        observer_tokens = self.observer.get_token_usage()
        probe_tokens = self.probe.get_token_usage() if self.probe else {"input_tokens": 0, "output_tokens": 0, "total_tokens": 0}
        executor_tokens = self.executor.get_token_usage() if self.executor else {"input_tokens": 0, "output_tokens": 0, "total_tokens": 0}
        
        total_input = observer_tokens["input_tokens"] + probe_tokens["input_tokens"] + executor_tokens["input_tokens"]
        total_output = observer_tokens["output_tokens"] + probe_tokens["output_tokens"] + executor_tokens["output_tokens"]
        total_all = observer_tokens["total_tokens"] + probe_tokens["total_tokens"] + executor_tokens["total_tokens"]
        
        self.logger.info(f"\nğŸ“Š Token Usage Statistics:")
        self.logger.info(f"  Observer   : {observer_tokens['total_tokens']:,} tokens (in: {observer_tokens['input_tokens']:,}, out: {observer_tokens['output_tokens']:,})")
        self.logger.info(f"  Probe      : {probe_tokens['total_tokens']:,} tokens (in: {probe_tokens['input_tokens']:,}, out: {probe_tokens['output_tokens']:,})")
        self.logger.info(f"  Executor   : {executor_tokens['total_tokens']:,} tokens (in: {executor_tokens['input_tokens']:,}, out: {executor_tokens['output_tokens']:,})")
        self.logger.info(f"  {'â”€' * 60}")
        self.logger.info(f"  Total      : {total_all:,} tokens (in: {total_input:,}, out: {total_output:,})")

    def save_execution_results(self, problem_id: str):
        """ä¿å­˜æ‰§è¡Œç»“æœåˆ°JSONæ–‡ä»¶"""
        # è·å–modelåç§°ï¼ˆç›´æ¥ä½¿ç”¨ï¼Œä¸åšæ›¿æ¢ï¼‰
        model_name = self.llm_config.llm_config.llm_model_name if hasattr(self, 'llm_config') and self.llm_config else "unknown"
        
        # æ”¯æŒæŒ‰è½®æ¬¡åˆ†å¼€ä¿å­˜ï¼ˆé€šè¿‡ ROUND ç¯å¢ƒå˜é‡ï¼‰
        round_num = os.environ.get("ROUND", "")
        if round_num:
            res_dir = f"./res/{model_name}-round{round_num}"
        else:
            res_dir = f"./res/{model_name}"
        os.makedirs(res_dir, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨problem_idï¼‰
        filename = f"{res_dir}/{problem_id}.json"

        # æ”¶é›† agent çš„ token ä½¿ç”¨æƒ…å†µ
        token_usage = {
            "observer": self.observer.get_token_usage() if self.observer else {"input_tokens": 0, "output_tokens": 0, "total_tokens": 0},
            "probe": self.probe.get_token_usage() if self.probe else {"input_tokens": 0, "output_tokens": 0, "total_tokens": 0},
            "executor": self.executor.get_token_usage() if self.executor else {"input_tokens": 0, "output_tokens": 0, "total_tokens": 0}
        }
        
        # è®¡ç®—æ€»è®¡
        total_input_tokens = sum(agent_usage["input_tokens"] for agent_usage in token_usage.values())
        total_output_tokens = sum(agent_usage["output_tokens"] for agent_usage in token_usage.values())
        total_tokens = sum(agent_usage["total_tokens"] for agent_usage in token_usage.values())
        
        token_usage["total"] = {
            "input_tokens": total_input_tokens,
            "output_tokens": total_output_tokens,
            "total_tokens": total_tokens
        }

        # å‡†å¤‡å®Œæ•´ç»“æœ
        full_results = {
            "problem_id": problem_id,
            "session_id": self.session_id,
            "timestamp": datetime.now().isoformat(),
            "task_info": self.task_info,
            "token_usage": token_usage,  # æ·»åŠ tokenä½¿ç”¨ç»Ÿè®¡
            "execution_results": self.execution_results,
            "evaluation_results": self.evaluation_results,  # æ·»åŠ è¯„ä¼°ç»“æœ
            "final_task_queue": [
                {
                    "task_name": task.task_name,
                    "status": task.status.value,
                    "target_agent": task.target_agent.value if task.target_agent else None,
                    "execution_rounds": task.execution_rounds,
                    "is_submit_task": task.is_submit_task
                }
                for task in (self.observer.task_queue if self.observer else [])
            ]
        }

        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(full_results, f, indent=2, ensure_ascii=False, default=str)

        return filename

    async def run(self,
                  problem_id: Optional[str] = None,
                  session_id: Optional[str] = None) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„é—®é¢˜è§£å†³æµç¨‹

        Args:
            problem_id: é—®é¢˜ID
            session_id: ä¼šè¯ID

        Returns:
            æ‰§è¡Œç»“æœ
        """
        try:
            # è®¾ç½®æ—¥å¿—æ–‡ä»¶
            model_name = self.llm_config.llm_config.llm_model_name
            if problem_id:
                FileLogHandler.set_log_file(problem_id, model_name)
            elif session_id:
                FileLogHandler.set_log_file(f"session_{session_id}", model_name)

            self.logger.info("\n" + "=" * 80)
            self.logger.info("ğŸš€ AI OPERATIONS PLATFORM STARTING")
            self.logger.info("=" * 80)

            # ä¿å­˜problem_id
            self.problem_id = problem_id

            # åˆå§‹åŒ–æˆ–è¿æ¥ä¼šè¯
            if self.env_client:
                if session_id:
                    result = self.env_client.connect_session(session_id)
                    self.logger.info(f"ğŸ“Œ Connected to session: {session_id}")
                elif problem_id:
                    # é‡è¯•init_problemï¼Œæœ€å¤š4æ¬¡
                    result = None
                    init_failed = False
                    for attempt in range(5):
                        try:
                            result = self.env_client.init_problem(problem_id)
                            if result:
                                self.logger.info(f"ğŸ“Œ Initialized problem: {problem_id} (attempt {attempt + 1})")
                                break
                            else:
                                self.logger.warning(f"âš ï¸ Attempt {attempt + 1}: Empty task description, retrying...")
                                if attempt < 3:  # ä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                                    continue
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ Attempt {attempt + 1} failed: {str(e)}")
                            if attempt < 3:  # ä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                                continue
                            else:
                                # æœ€åä¸€æ¬¡å°è¯•ä¹Ÿå¤±è´¥ï¼Œæ ‡è®°åˆå§‹åŒ–å¤±è´¥
                                init_failed = True
                                self.logger.error(f"âŒ Failed to initialize problem {problem_id} after 4 attempts")
                                break
                    
                    # æ£€æŸ¥åˆå§‹åŒ–æ˜¯å¦å¤±è´¥
                    if init_failed or not result or result.get('task_description', 'N/A') == 'N/A':
                        self.logger.error(f"âŒ Terminating problem {problem_id} due to initialization failure")
                        self.logger.error(f"ğŸ“ Log saved, but result file will NOT be saved")
                        # ç›´æ¥è¿”å›å¤±è´¥çŠ¶æ€ï¼Œä¸ä¿å­˜ res æ–‡ä»¶
                        return {
                            "success": False,
                            "error": f"Failed to initialize problem {problem_id} after 4 attempts",
                            "session_id": None,
                            "initialization_failed": True  # æ ‡è®°ä¸ºåˆå§‹åŒ–å¤±è´¥
                        }
                else:
                    raise ValueError("Either problem_id or session_id required")

                self.session_id = self.env_client.get_session_id()
                task_info = result

                # æ˜¾ç¤ºä»»åŠ¡ä¿¡æ¯
                self.logger.info(f"\nğŸ“‹ Task Description:")
                task_desc = task_info.get('task_description', 'N/A')
                if len(task_desc) > 500:
                    self.logger.info(f"{task_desc[:500]}...")
                else:
                    self.logger.info(f"{task_desc}")

            else:
                # æ¨¡æ‹Ÿæ¨¡å¼
                self.session_id = session_id or "test-session"
                task_info = {
                    "task_description": "Test task in simulation mode",
                    "available_actions": {},
                    "instructions": "Test instructions"
                }

            # åˆå§‹åŒ–æ™ºèƒ½ä½“ï¼ˆåŒ…æ‹¬åˆ›å»ºå­ä»»åŠ¡é˜Ÿåˆ—ï¼‰
            self.logger.info(f"\nğŸ¤– Initializing agents and task queue...")
            self._initialize_agents(task_info)

            # ä¸»å¾ªç¯ - æ‰§è¡Œå­ä»»åŠ¡é˜Ÿåˆ—
            for iteration in range(1, self.max_iterations + 1):
                self.current_iteration = iteration

                # è¿è¡Œå•æ¬¡è¿­ä»£
                result = await self.run_iteration(iteration)

                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                if result.get("status") == "completed":
                    self.logger.info(f"\n{'=' * 80}")
                    self.logger.info(f"ğŸ“Š TASK COMPLETED")
                    self.logger.info(f"{'=' * 80}")

                    # æ˜¾ç¤ºæœ€ç»ˆä»»åŠ¡é˜Ÿåˆ—çŠ¶æ€
                    self.logger.info(f"\nğŸ“‹ Final Task Queue Status:")
                    for i, task in enumerate(self.observer.task_queue, 1):
                        # ä½¿ç”¨ä¸ObserverAgentç›¸åŒçš„çŠ¶æ€å›¾æ ‡é€»è¾‘
                        status_icon = {
                            TaskStatus.PENDING: "â¸",
                            TaskStatus.EXECUTING: "â–¶ï¸",
                            TaskStatus.COMPLETED: "âœ…",
                            TaskStatus.FAILED: "âŒ",
                            TaskStatus.SKIPPED: "â­"
                        }.get(task.status, "â“")
                        self.logger.info(f"  {status_icon} {task.task_name}")

                    # æ˜¾ç¤ºtokenç»Ÿè®¡å¹¶ä¿å­˜ç»“æœ
                    self.log_token_usage()
                    if problem_id:
                        result_file = self.save_execution_results(problem_id)
                        self.logger.info(f"ğŸ“ Results saved to: {result_file}")

                    # æ¸…ç†ä¼šè¯
                    if self.env_client:
                        try:
                            self.env_client.cleanup_session()
                            self.logger.info("ğŸ§¹ Session cleaned up successfully")
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ Failed to cleanup session: {e}")

                    # åˆ¤æ–­çœŸæ­£çš„æˆåŠŸï¼ševaluation_resultså¿…é¡»éç©ºä¸”success == true æˆ– "Detection Accuracy" == "Correct"
                    is_success = False
                    if self.evaluation_results:  # ç¡®ä¿evaluation_resultsä¸ä¸ºç©º
                        is_success = (
                            self.evaluation_results.get('success') == True or
                            self.evaluation_results.get('Detection Accuracy') == 'Correct'
                        )
                    
                    if not is_success:
                        self.logger.warning(f"âš ï¸ Task completed but evaluation shows failure or empty results")

                    return {
                        "success": is_success,
                        "iterations": iteration,
                        "solution": result.get("solution", ""),
                        "session_id": self.session_id,
                        "evaluation_results": self.evaluation_results  # æ·»åŠ è¯„ä¼°ç»“æœ
                    }

            # è¾¾åˆ°æœ€å¤§è¿­ä»£ï¼ˆåº”è¯¥åœ¨æœ€åä¸€è½®æäº¤ï¼‰
            self.logger.warning(f"\nâ° Reached maximum iterations")

            # å°è¯•æœ€ç»ˆæäº¤
            if self.env_client:
                submit_result = self.execute_action("submit()")
                if self.env_client.is_problem_solved():
                    # æ˜¾ç¤ºtokenç»Ÿè®¡å¹¶ä¿å­˜ç»“æœ
                    self.log_token_usage()
                    if problem_id:
                        result_file = self.save_execution_results(problem_id)
                        self.logger.info(f"ğŸ“ Results saved to: {result_file}")

                    # æ¸…ç†ä¼šè¯
                    if self.env_client:
                        try:
                            self.env_client.cleanup_session()
                            self.logger.info("ğŸ§¹ Session cleaned up successfully")
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ Failed to cleanup session: {e}")

                    # åˆ¤æ–­çœŸæ­£çš„æˆåŠŸï¼ševaluation_resultså¿…é¡»éç©ºä¸”success == true æˆ– "Detection Accuracy" == "Correct"
                    is_success = False
                    if self.evaluation_results:  # ç¡®ä¿evaluation_resultsä¸ä¸ºç©º
                        is_success = (
                            self.evaluation_results.get('success') == True or
                            self.evaluation_results.get('Detection Accuracy') == 'Correct'
                        )
                    
                    if not is_success:
                        self.logger.warning(f"âš ï¸ Task completed but evaluation shows failure or empty results")

                    return {
                        "success": is_success,
                        "iterations": self.max_iterations,
                        "message": "Solved at final submission",
                        "session_id": self.session_id,
                        "evaluation_results": self.evaluation_results  # æ·»åŠ è¯„ä¼°ç»“æœ
                    }

            # æ˜¾ç¤ºtokenç»Ÿè®¡å¹¶ä¿å­˜ç»“æœï¼ˆå³ä½¿å¤±è´¥ï¼‰
            self.log_token_usage()
            if problem_id:
                result_file = self.save_execution_results(problem_id)
                self.logger.info(f"ğŸ“ Results saved to: {result_file}")

            return {
                "success": False,
                "iterations": self.max_iterations,
                "message": "Maximum iterations reached",
                "session_id": self.session_id,
                "evaluation_results": self.evaluation_results  # æ·»åŠ è¯„ä¼°ç»“æœ
            }


        except Exception as e:
            self.logger.error(f"\nâŒ Fatal error: {str(e)}")
            self.logger.error(traceback.format_exc())

            # æ˜¾ç¤ºtokenç»Ÿè®¡å¹¶ä¿å­˜é”™è¯¯ç»“æœ
            self.log_token_usage()
            if problem_id:
                try:
                    result_file = self.save_execution_results(problem_id)
                    self.logger.info(f"ğŸ“ Results saved to: {result_file}")
                except:
                    pass

            return {
                "success": False,
                "error": str(e),
                "traceback": traceback.format_exc(),
                "session_id": self.session_id,
                "evaluation_results": self.evaluation_results  # æ·»åŠ è¯„ä¼°ç»“æœ
            }
        finally:
            # Close log file
            FileLogHandler.close()
