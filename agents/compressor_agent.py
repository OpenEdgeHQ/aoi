# agents/compressor_agent.py

from typing import Dict, Any, List, Optional
from datetime import datetime
import json
import asyncio
import re

from aworld.agents.llm_agent import Agent
from aworld.config.conf import AgentConfig
from aworld.runner import Runners

from agents.base_agent import BaseAgent
from memory.memory_manager import MemoryManager
from memory.memory_item import (
    AgentType, RawContextItem, CompressedContextItem,
    MemoryType, SubTaskItem
)
from prompts.loader import get_prompt_loader



class CompressorAgent(BaseAgent):
    """å‹ç¼©å™¨æ™ºèƒ½ä½“ - åŸºäºLLMçš„æ™ºèƒ½ä¸Šä¸‹æ–‡å‹ç¼©"""

    def __init__(self,
                 llm_config: AgentConfig,
                 memory_manager: MemoryManager,
                 window_size: int = 8000,
                 overlap_size: int = 2000,
                 min_compress_length: int = 500,
                 max_output_tokens: int = 5000,
                 max_context_tokens: int = 35000):
        """
        åˆå§‹åŒ–å‹ç¼©å™¨æ™ºèƒ½ä½“

        Args:
            llm_config: LLMé…ç½®
            memory_manager: å†…å­˜ç®¡ç†å™¨
            window_size: æ»‘åŠ¨çª—å£å¤§å°ï¼ˆtokensï¼‰
            overlap_size: çª—å£é‡å å¤§å°ï¼ˆtokensï¼‰
            min_compress_length: æœ€å°å‹ç¼©é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰
            max_output_tokens: ç›®æ ‡å‹ç¼©åçš„æœ€å¤§tokenæ•°
            max_context_tokens: æ¨¡å‹æœ€å¤§è¾“å…¥tokenæ•°
        """
        self.prompt_loader = get_prompt_loader()
        self.min_compress_length = min_compress_length
        self.max_output_tokens = max_output_tokens
        self.max_context_tokens = max_context_tokens

        super().__init__(
            name="Compressor Agent",
            agent_type=AgentType.COMPRESSOR,
            llm_config=llm_config,
            memory_manager=memory_manager,
            max_iterations=1
        )

        self.window_size = window_size
        self.overlap_size = overlap_size
        self.model_name = self._get_model_name()

    def _get_model_name(self) -> str:
        """å®‰å…¨åœ°è·å–æ¨¡å‹åç§°"""
        possible_attrs = ['model_name', 'model', 'llm_model', 'llm_model_name']
        for attr in possible_attrs:
            if hasattr(self.llm_config, attr):
                value = getattr(self.llm_config, attr)
                if value:
                    return value
        return "unknown_model"

    def _initialize_llm_agent(self) -> Agent:
        """åˆå§‹åŒ–LLMæ™ºèƒ½ä½“"""
        return Agent(
            name=self.name,
            conf=self.llm_config,
            system_prompt=self._get_system_prompt()
        )

    def _get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯ - æ™ºèƒ½å‹ç¼©ç‰ˆæœ¬"""
        return self.prompt_loader.get_prompt(
            agent_type="compressor",
            prompt_type="system"
        )

    def _prepare_input(self,
                       instruction: str,
                       context: Dict[str, Any],
                       **kwargs) -> str:
        """
        å‡†å¤‡LLMè¾“å…¥ - å®ç°æŠ½è±¡æ–¹æ³•

        Args:
            instruction: ä»»åŠ¡æŒ‡ä»¤ï¼ˆå‹ç¼©å™¨å¯èƒ½ä¸ä½¿ç”¨ï¼‰
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            æ ¼å¼åŒ–çš„è¾“å…¥å­—ç¬¦ä¸²
        """
        raw_output = kwargs.get("raw_output", context.get("raw_data", ""))
        source_agent = kwargs.get("source_agent", "unknown")

        # æ£€æŸ¥è¾“å‡ºé•¿åº¦
        output_length = len(raw_output)

        # ä¼°ç®—tokenæ•°ï¼ˆç²—ç•¥ä¼°ç®—ï¼š1 token â‰ˆ 4 å­—ç¬¦ï¼‰
        estimated_tokens = output_length // 4
        target_tokens = self.max_output_tokens

        # ä½¿ç”¨prompt loaderåŠ è½½æ¨¡æ¿
        params = {
            "output_length": output_length,
            "raw_output": raw_output,
            "target_tokens": target_tokens,
            "estimated_tokens": estimated_tokens
        }

        return self.prompt_loader.get_prompt(
            agent_type="compressor",
            prompt_type="user",
            **params
        )

    def _process_decision(self,
                          decision: Dict[str, Any],
                          context: Dict[str, Any]) -> str:
        """
        å¤„ç†å†³ç­–ç»“æœ - å®ç°æŠ½è±¡æ–¹æ³•

        Args:
            decision: LLMå†³ç­–ç»“æœ
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            å‹ç¼©åçš„è¾“å‡ºå­—ç¬¦ä¸²
        """
        output = decision.get("compressed_output", decision.get("raw_response", ""))

        if not output:
            # å¦‚æœæ²¡æœ‰è¾“å‡ºï¼Œç›´æ¥æŠ¥é”™ï¼Œä¸ä½¿ç”¨fallback
            raise ValueError("Compression failed: LLM returned empty output")

        # è®°å½•åˆ°å†…å­˜
        if self.memory_manager and output:
            self._store_compressed_context(output, context)

        self.agent_logger.info(f"Compression completed, output length: {len(output)}")
        return output

    def _get_fallback_decision(self) -> Dict[str, Any]:
        """
        è·å–å¤‡ç”¨å†³ç­– - å®ç°æŠ½è±¡æ–¹æ³•
        æ³¨æ„ï¼šæ­¤å‡½æ•°ä¸åº”è¢«è°ƒç”¨ï¼Œå› ä¸ºæˆ‘ä»¬ç›´æ¥æŠ›å‡ºé”™è¯¯è€Œä¸æ˜¯ä½¿ç”¨fallback
        
        Returns:
            å¤‡ç”¨å†³ç­–å­—å…¸
        """
        raise NotImplementedError("Fallback decisions are not supported. Compression should either succeed or raise an error.")

    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """
        è§£æLLMå“åº” - è¦†ç›–çˆ¶ç±»æ–¹æ³•ä»¥é€‚åº”å‹ç¼©å™¨ç‰¹æ€§

        Args:
            response: LLMåŸå§‹å“åº”

        Returns:
            è§£æåçš„å†³ç­–å­—å…¸
        """
        # å‹ç¼©å™¨çš„å“åº”ç›´æ¥ä½œä¸ºå‹ç¼©è¾“å‡º
        return {
            "compressed_output": response,
            "raw_response": response
        }

    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°ï¼ˆç²—ç•¥ä¼°ç®—ï¼š1 token â‰ˆ 4 å­—ç¬¦ï¼‰"""
        return len(text) // 4

    def _split_text_into_windows(self, text: str, window_size_tokens: int, overlap_tokens: int = 0) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªæ»‘åŠ¨çª—å£
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            window_size_tokens: æ¯ä¸ªçª—å£çš„å¤§å°ï¼ˆtokensï¼‰
            overlap_tokens: çª—å£ä¹‹é—´çš„é‡å å¤§å°ï¼ˆtokensï¼‰
            
        Returns:
            çª—å£æ–‡æœ¬åˆ—è¡¨
        """
        # å°†æ–‡æœ¬æŒ‰å­—ç¬¦åˆ†å‰²
        chars_per_token = 4  # ç²—ç•¥ä¼°ç®—
        window_size_chars = window_size_tokens * chars_per_token
        overlap_chars = overlap_tokens * chars_per_token
        step_size = window_size_chars - overlap_chars
        
        if step_size <= 0:
            step_size = window_size_chars
        
        windows = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            end = min(start + window_size_chars, text_length)
            window_text = text[start:end]
            windows.append(window_text)
            
            # å¦‚æœå·²ç»åˆ°è¾¾æ–‡æœ¬æœ«å°¾ï¼Œé€€å‡º
            if end >= text_length:
                break
                
            start += step_size
        
        self.agent_logger.info(f"  ğŸªŸ Split text into {len(windows)} windows (window_size={window_size_tokens} tokens, "
                        f"overlap={overlap_tokens} tokens)")
        
        return windows

    async def _sliding_window_compress(self, text: str, item: RawContextItem, 
                                      target_tokens_per_window: int) -> str:
        """
        ä½¿ç”¨æ— é‡å¤æ»‘çª—å¯¹è¶…é•¿æ–‡æœ¬è¿›è¡Œå‹ç¼©
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            item: åŸå§‹ä¸Šä¸‹æ–‡é¡¹
            target_tokens_per_window: æ¯ä¸ªçª—å£å‹ç¼©åçš„ç›®æ ‡tokenæ•°
            
        Returns:
            å‹ç¼©åçš„æ–‡æœ¬
        """
        # 1. å°†æ–‡æœ¬åˆ†å‰²æˆçª—å£ï¼ˆçª—å£å¤§å°ä¸ºmax_context_tokensï¼Œæ— é‡å ï¼‰
        windows = self._split_text_into_windows(
            text, 
            window_size_tokens=self.max_context_tokens,
            overlap_tokens=0  # æ— é‡å æ»‘çª—
        )
        
        self.agent_logger.info(f"  ğŸ—œï¸  Compressing {len(windows)} windows with sliding window strategy...")
        
        # 2. å‹ç¼©æ¯ä¸ªçª—å£
        compressed_windows = []
        for idx, window_text in enumerate(windows, 1):
            self.agent_logger.info(f"    [{idx}/{len(windows)}] Compressing window {idx}...")
            
            # ä¸ºæ¯ä¸ªçª—å£è°ƒç”¨LLMå‹ç¼©ï¼Œå¦‚æœå¤±è´¥ç›´æ¥æŠ›å‡ºå¼‚å¸¸
            compressed = await self._intelligent_compress_single(
                window_text, 
                item, 
                target_tokens=target_tokens_per_window
            )
            compressed_windows.append(compressed)
        
        # 3. åˆå¹¶æ‰€æœ‰å‹ç¼©åçš„çª—å£
        final_result = "\n\n".join([
            f"[Window {i+1}/{len(compressed_windows)}]\n{content}" 
            for i, content in enumerate(compressed_windows)
        ])
        
        return final_result


    async def _structure_raw_items_with_compression(self, raw_items: List[RawContextItem]) -> str:
        """
        å°†åŸå§‹ä¸Šä¸‹æ–‡é¡¹ç»“æ„åŒ–ä¸ºmarkdownæ ¼å¼ï¼Œæ™ºèƒ½æ‰¹é‡å‹ç¼©Outputéƒ¨åˆ†
        
        æ–°å‹ç¼©é€»è¾‘ï¼š
        1. éå†æ¯ä¸ªroundç»“æœ
        2. å¯¹roundç»“æœè¿›è¡Œæ–‡æœ¬å»é‡
        3. å¦‚æœå»é‡åç»“æœå°äºé˜ˆå€¼threshold_per_roundï¼Œç›´æ¥ä¿å­˜
        4. å¦‚æœå»é‡åç»“æœå¤§äºé˜ˆå€¼ï¼š
           - å¦‚æœå°äºæ¨¡å‹æœ€å¤§è¾“å…¥é˜ˆå€¼max_context_tokensï¼Œç›´æ¥å‹ç¼©ï¼Œç›®æ ‡é•¿åº¦=max_output_tokens//n_round
           - å¦‚æœå¤§äºæ¨¡å‹æœ€å¤§è¾“å…¥é˜ˆå€¼max_context_tokensï¼Œä½¿ç”¨æ»‘çª—å‹ç¼©ï¼Œç›®æ ‡é•¿åº¦=max_output_tokens//n_round//n_windows
        """
        if not raw_items:
            return "# No Raw Data Available\n"

        # æŒ‰æ—¶é—´æ’åº
        sorted_items = sorted(raw_items, key=lambda x: (x.round_number, x.created_at))

        # æå–æ—¶é—´èŒƒå›´
        time_range = f"{sorted_items[0].created_at.strftime('%H:%M:%S')} - {sorted_items[-1].created_at.strftime('%H:%M:%S')}"

        # è®¡ç®—æ¯ä¸ªroundçš„é˜ˆå€¼
        n_rounds = len(sorted_items)
        threshold_per_round = self.max_output_tokens // n_rounds if n_rounds > 0 else self.max_output_tokens
        
        self.agent_logger.info(f"ğŸ“Š Compression strategy: {n_rounds} rounds")
        self.agent_logger.info(f"   - Threshold per round: {threshold_per_round} tokens")
        self.agent_logger.info(f"   - Max context tokens: {self.max_context_tokens} tokens")
        self.agent_logger.info(f"   - Max output tokens: {self.max_output_tokens} tokens")

        # ç¬¬ä¸€éï¼šé¢„å¤„ç†æ‰€æœ‰itemsï¼Œåˆ¤æ–­å‹ç¼©ç­–ç•¥
        items_data = []
        
        for idx, item in enumerate(sorted_items, 1):
            # 1. å¤„ç†è¾“å‡ºå¹¶ä¼°ç®—tokensï¼ˆå»é‡å·²åœ¨probe/executorä¸­å®Œæˆï¼‰
            output = self._process_output(item.raw_output)
            output_tokens = self._estimate_tokens(output)
            
            # 2. åˆ¤æ–­å‹ç¼©ç­–ç•¥
            strategy = "keep"  # é»˜è®¤ç­–ç•¥ï¼šä¿æŒä¸å˜
            target_tokens = threshold_per_round
            
            if output_tokens <= threshold_per_round:
                # å°äºé˜ˆå€¼ï¼Œç›´æ¥ä¿å­˜
                strategy = "keep"
                self.agent_logger.info(f"  Round {item.round_number}: {output_tokens} tokens â‰¤ {threshold_per_round} â†’ keep")
                
            elif output_tokens < self.max_context_tokens:
                # å¤§äºé˜ˆå€¼ä½†å°äºæ¨¡å‹è¾“å…¥é™åˆ¶ï¼Œç›´æ¥å‹ç¼©
                strategy = "compress"
                target_tokens = self.max_output_tokens // n_rounds
                self.agent_logger.info(f"  Round {item.round_number}: {output_tokens} tokens â†’ direct compress to {target_tokens} tokens")
                
            else:
                # å¤§äºæ¨¡å‹è¾“å…¥é™åˆ¶ï¼Œä½¿ç”¨æ»‘çª—å‹ç¼©
                strategy = "sliding_window"
                # è®¡ç®—éœ€è¦å¤šå°‘ä¸ªçª—å£
                n_windows = (output_tokens + self.max_context_tokens - 1) // self.max_context_tokens
                target_tokens = (self.max_output_tokens // n_rounds) // n_windows
                self.agent_logger.info(f"  Round {item.round_number}: {output_tokens} tokens â†’ sliding window compress "
                               f"({n_windows} windows, target {target_tokens} tokens per window)")

            item_data = {
                'index': idx,
                'item': item,
                'output': output,
                'output_tokens': output_tokens,
                'strategy': strategy,
                'target_tokens': target_tokens
            }
            items_data.append(item_data)

        # ç¬¬äºŒéï¼šæ ¹æ®ç­–ç•¥å¤„ç†æ¯ä¸ªround
        compressed_outputs = {}
        
        for item_data in items_data:
            idx = item_data['index']
            round_num = item_data['item'].round_number
            strategy = item_data['strategy']
            
            if strategy == "keep":
                # ç›´æ¥ä¿å­˜åŸå§‹è¾“å‡º
                compressed_outputs[idx] = item_data['output']
                
            elif strategy == "compress":
                # ç›´æ¥å‹ç¼©ï¼Œå¤±è´¥ç›´æ¥æŠ›å‡ºå¼‚å¸¸
                self.agent_logger.info(f"   ğŸ—œï¸  [{idx}/{n_rounds}] Round {round_num}: direct compression...")
                compressed = await self._intelligent_compress_single(
                    item_data['output'], 
                    item_data['item'],
                    target_tokens=item_data['target_tokens']
                )
                compressed_outputs[idx] = compressed
                self.agent_logger.info(f"   âœ… Round {round_num} compressed successfully")
                    
            elif strategy == "sliding_window":
                # æ»‘çª—å‹ç¼©ï¼Œå¤±è´¥ç›´æ¥æŠ›å‡ºå¼‚å¸¸
                self.agent_logger.info(f"   ğŸªŸ [{idx}/{n_rounds}] Round {round_num}: sliding window compression...")
                compressed = await self._sliding_window_compress(
                    item_data['output'],
                    item_data['item'],
                    target_tokens_per_window=item_data['target_tokens']
                )
                compressed_outputs[idx] = compressed
                self.agent_logger.info(f"   âœ… Round {round_num} sliding window compressed successfully")

        # ç¬¬ä¸‰éï¼šæ„å»ºæœ€ç»ˆè¾“å‡º - åŒ…å«å‘½ä»¤å’Œå‹ç¼©åçš„å†…å®¹
        output_parts = []
        
        for item_data in items_data:
            idx = item_data['index']
            item = item_data['item']
            
            # è·å–æœ€ç»ˆè¾“å‡ºï¼ˆå‹ç¼©åçš„å†…å®¹ï¼‰
            final_output = compressed_outputs.get(idx, item_data['output'])
            
            # å¦‚æœæœ‰å¤šä¸ªroundï¼Œç”¨åˆ†éš”ç¬¦åˆ†å¼€
            if len(items_data) > 1:
                output_parts.append(f"[Round {item.round_number}]")
            
            # æ·»åŠ å‘½ä»¤ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if hasattr(item, 'command') and item.command:
                output_parts.append(f"**Command**: `{item.command}`")
                output_parts.append("")  # ç©ºè¡Œåˆ†éš”
            
            # æ·»åŠ ç»“æœæ ‡é¢˜å’Œå†…å®¹
            output_parts.append("**Result**:")
            output_parts.append(final_output)
            
            # æ·»åŠ æ¢è¡Œåˆ†éš”
            if idx < len(items_data):
                output_parts.append("\n")

        # è¿”å›åŒ…å«å‘½ä»¤å’Œç»“æœçš„å®Œæ•´å†…å®¹
        return "\n".join(output_parts)

    async def _intelligent_compress_single(self, output_text: str, item: RawContextItem, 
                                          target_tokens: Optional[int] = None) -> str:
        """
        ä½¿ç”¨LLMæ™ºèƒ½å‹ç¼©å•ä¸ªè¾“å‡º
        
        Args:
            output_text: è¦å‹ç¼©çš„æ–‡æœ¬
            item: åŸå§‹ä¸Šä¸‹æ–‡é¡¹
            target_tokens: ç›®æ ‡tokenæ•°ï¼ˆå¦‚æœæŒ‡å®šï¼‰
            
        Returns:
            å‹ç¼©åçš„æ–‡æœ¬
            
        Raises:
            Exception: å¦‚æœå‹ç¼©å¤±è´¥
        """
        # 1. æ£€æŸ¥é•¿åº¦ï¼Œå†³å®šæ˜¯å¦éœ€è¦å‹ç¼©
        if len(output_text) < self.min_compress_length:
            self.agent_logger.info(f"Text length ({len(output_text)}) below threshold ({self.min_compress_length}), returning as-is")
            return output_text
        
        # 2. æ–‡æœ¬é•¿åº¦è¶…è¿‡é˜ˆå€¼ï¼Œä½¿ç”¨LLMå‹ç¼©
        self.agent_logger.info(f"Text length ({len(output_text)}) exceeds threshold ({self.min_compress_length}), compressing with LLM")
        
        # é€šè¿‡processæ–¹æ³•è°ƒç”¨LLMï¼ˆä½¿ç”¨BaseAgentçš„æ¡†æ¶ï¼‰
        # å¦‚æœå‹ç¼©å¤±è´¥ï¼Œä¼šç›´æ¥æŠ›å‡ºå¼‚å¸¸
        compressed = await self.process(
            task_instruction="",  # å‹ç¼©å™¨ä¸éœ€è¦instruction
            context={"raw_data": output_text, "target_tokens": target_tokens},
            raw_output=output_text,
            source_agent=item.source_agent.value if item.source_agent else "unknown"
        )
        return compressed

    def _process_output(self, output: Any) -> str:
        """
        å¤„ç†è¾“å‡ºæ•°æ®ï¼Œæ”¯æŒå­—å…¸ã€åˆ—è¡¨å’Œå­—ç¬¦ä¸²

        Args:
            output: åŸå§‹è¾“å‡º

        Returns:
            å¤„ç†åçš„å­—ç¬¦ä¸²
        """
        if isinstance(output, dict):
            try:
                return json.dumps(output, indent=2, ensure_ascii=False)
            except:
                return str(output)
        elif isinstance(output, list):
            try:
                return json.dumps(output, indent=2, ensure_ascii=False)
            except:
                return str(output)
        elif isinstance(output, str):
            return output
        else:
            return str(output)

    def _store_compressed_context(self, output: str, context: Dict[str, Any]):
        """å­˜å‚¨å‹ç¼©ä¸Šä¸‹æ–‡åˆ°å†…å­˜"""
        # ç®€åŒ–ï¼šä¸å†è¿›è¡Œç¡¬ç¼–ç åˆ†æï¼Œç›´æ¥å­˜å‚¨
        key_findings = []

        # è®¡ç®—å‹ç¼©ç‡
        original_size = len(context.get("raw_data", ""))
        compressed_size = len(output)
        compression_ratio = 0.0

        if original_size > 0 and compressed_size < original_size:
            compression_ratio = 1.0 - (compressed_size / original_size)

        compressed_item = CompressedContextItem(
            source_items=context.get("source_ids", []),
            compression_ratio=compression_ratio,
            original_size=original_size,
            compressed_size=compressed_size,
            summary=f"Compressed from {original_size} to {compressed_size} chars",
            key_findings=key_findings,
            anomaly_indicators={},
            timeline=self._extract_timeline(context),
            recommendations=self._generate_recommendations(),
            semantic_tags=[],  # è®©LLMè‡ªå·±æå–semanticä¿¡æ¯
            confidence_score=0.95,
            compression_model=self.model_name,
            compression_prompt="Enhanced error-preserving compression",
            metadata={
                "session_id": context.get("session_id", ""),
                "compression_timestamp": datetime.now().isoformat()
            }
        )

        self.memory_manager.add_item(compressed_item, self.agent_type)
        return compressed_item.id

    def _extract_timeline(self, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æå–æ—¶é—´çº¿ä¿¡æ¯"""
        timeline = []
        raw_items = context.get("raw_items", [])

        for item in raw_items:
            if isinstance(item, RawContextItem):
                timeline.append({
                    "timestamp": item.created_at.isoformat(),
                    "round": item.round_number,
                    "event": f"{item.source_agent.value if item.source_agent else 'unknown'}: {item.command[:50] if item.command else 'N/A'}",
                    "success": getattr(item, 'success', True)
                })

        return sorted(timeline, key=lambda x: x["timestamp"])

    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆå»ºè®®"""
        return ["Review compressed output for issues"]


    async def compress_context(self,
                               raw_data: str = "",
                               source_agent: str = "",
                               round_info: str = "",
                               source_ids: List[str] = None,
                               session_id: str = None,
                               raw_items: List[RawContextItem] = None) -> str:
        """
        å‹ç¼©ä¸Šä¸‹æ–‡ - ä¸»è¦æ¥å£


        Raises:
            Exception: å¦‚æœå‹ç¼©å¤±è´¥
        """
        # å¦‚æœæä¾›äº†raw_itemsï¼Œä½¿ç”¨æ–°çš„ç»“æ„åŒ–å¤„ç†
        if raw_items:
            result = await self._structure_raw_items_with_compression(raw_items)
            source_ids = [item.id for item in raw_items]

        # å¦‚æœæ²¡æœ‰åŸå§‹æ•°æ®ï¼Œä»å†…å­˜è·å–
        elif not raw_data and source_ids and self.memory_manager:
            raw_items = []
            for item_id in source_ids:
                item = self.memory_manager.get_item(item_id, self.agent_type)
                if isinstance(item, RawContextItem):
                    raw_items.append(item)

            if raw_items:
                result = await self._structure_raw_items_with_compression(raw_items)
            else:
                result = "[NO DATA TO COMPRESS]"

        # å¦‚æœæœ‰åŸå§‹æ•°æ®ä½†æ²¡æœ‰raw_items
        elif raw_data:
            # æ£€æŸ¥é•¿åº¦å†³å®šæ˜¯å¦å‹ç¼©
            if len(raw_data) < self.min_compress_length:
                result = raw_data
            else:
                # ç›´æ¥æŠ¥é”™ï¼Œä¸å†ä½¿ç”¨fallback
                raise ValueError(f"Raw data length ({len(raw_data)}) exceeds threshold but raw_items not provided. Cannot compress without RawContextItem.")
        else:
            result = "[NO DATA TO COMPRESS]"

        # å­˜å‚¨å‹ç¼©ç»“æœåˆ°å†…å­˜
        if self.memory_manager and result != "[NO DATA TO COMPRESS]":
            context = {
                "raw_data": raw_data or "",
                "source_ids": source_ids or [],
                "session_id": session_id,
                "raw_items": raw_items or []
            }
            self._store_compressed_context(result, context)

        return result

    async def compressor_run(self, session_id: str, current_subtask: Optional[SubTaskItem] = None) -> str:
        """
        ä¸»è¦æ¥å£å‡½æ•° - æ ¹æ®session_idè·å–å¹¶å‹ç¼©æ•°æ®
        
        Args:
            session_id: ä¼šè¯ID
            current_subtask: å½“å‰å­ä»»åŠ¡ï¼Œç”¨äºè·å– iteration ä¿¡æ¯
        """
        try:
            # ä» current_subtask è·å– iteration
            current_iteration = current_subtask.iteration_number if current_subtask else None
            
            self.agent_logger.info(f"ğŸ—œï¸ Starting compression for session: {session_id}")

            if not self.memory_manager:
                return "[ERROR: No memory manager available]"

            # æŸ¥è¯¢è¯¥sessionçš„åŸå§‹ä¸Šä¸‹æ–‡ï¼ˆåªæŸ¥è¯¢å½“å‰iterationçš„æ•°æ®ï¼Œé¿å…é‡å¤å‹ç¼©ï¼‰
            # RAW_CONTEXT æ˜¯ç”± Probe å’Œ Executor äº§ç”Ÿçš„ï¼Œéœ€è¦æŸ¥è¯¢è¿™ä¸¤ä¸ª agent_type
            if current_iteration is not None:
                # åªæŸ¥è¯¢å½“å‰ iteration çš„æ•°æ®
                filters = {
                    "metadata": lambda x: (
                        isinstance(x, dict) and 
                        x.get("session_id") == session_id and 
                        x.get("iteration") == current_iteration
                    )
                }
                self.agent_logger.info(f"ğŸ” Querying RAW_CONTEXT for iteration {current_iteration} only")
            else:
                # æŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼ˆç”¨äºç‰¹æ®Šæƒ…å†µï¼Œä½†é€šå¸¸åº”è¯¥æœ‰ current_subtaskï¼‰
                filters = {
                    "metadata": lambda x: x.get("session_id") == session_id if isinstance(x, dict) else False
                }
                self.agent_logger.info(f"ğŸ” Querying all RAW_CONTEXT for session")

            raw_items = []
            
            # æŸ¥è¯¢ RAW_CONTEXT (ä½¿ç”¨ COMPRESSOR ä½œä¸ºæŸ¥è¯¢è€…ï¼Œå› ä¸ºå®ƒæœ‰READæƒé™)
            # ç„¶åé€šè¿‡ filters å’Œ source_agent å±æ€§æ¥è¿‡æ»¤æ•°æ®
            all_raw_items = self.memory_manager.query_items(
                agent_type=AgentType.COMPRESSOR,  # æŸ¥è¯¢è€…ï¼ˆæœ‰READæƒé™ï¼‰
                memory_type=MemoryType.RAW_CONTEXT,
                filters=filters,
                limit=1000,
                sort_by="created_at",
                descending=False
            )
            self.agent_logger.info(f"ğŸ“Š Query returned {len(all_raw_items)} total RAW_CONTEXT items")
            
            # æ‰‹åŠ¨è¿‡æ»¤å‡º Probe å’Œ Executor çš„æ•°æ®
            for item in all_raw_items:
                if hasattr(item, 'source_agent') and item.source_agent in [AgentType.PROBE, AgentType.EXECUTOR]:
                    raw_items.append(item)
            
            self.agent_logger.info(f"ğŸ“Š Filtered to {len(raw_items)} items from Probe/Executor")
            
            # æŒ‰ created_at æ’åº
            raw_items.sort(key=lambda x: x.created_at if hasattr(x, 'created_at') else 0)

            if not raw_items:
                self.agent_logger.warning(f"âš ï¸ No data found for session: {session_id}")
                return f"[NO DATA FOUND FOR SESSION: {session_id}]"

            # æ·»åŠ å‹ç¼©å‰ç»Ÿè®¡æ—¥å¿—
            total_size = sum(len(str(item.raw_output)) for item in raw_items)
            self.agent_logger.info(f"ğŸ“Š Found {len(raw_items)} items, total size: {total_size} chars")

            # æ‰§è¡Œæ™ºèƒ½å‹ç¼©
            compressed_result = await self.compress_context(
                raw_data="",
                source_agent="compressor_run",
                round_info=f"Batch compression for session {session_id}",
                source_ids=None,
                session_id=session_id,
                raw_items=raw_items
            )

            # æ·»åŠ å‹ç¼©åæ—¥å¿—
            compressed_size = len(compressed_result)
            compression_ratio = 1.0 - (compressed_size / total_size) if total_size > 0 else 0
            self.agent_logger.info(f"âœ… Compression completed: {compressed_size} chars (ratio: {compression_ratio:.2%})")

            return compressed_result

        except Exception as e:
            self.logger.error(f"Error in compressor_run: {e}", exc_info=True)
            return f"[ERROR: {str(e)}]"
