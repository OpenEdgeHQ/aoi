from typing import Dict, Any, Optional, List, Callable
from datetime import datetime
import json
import re

from aworld.agents.llm_agent import Agent
from aworld.config.conf import AgentConfig
from agents.base_agent import BaseAgent
from agents.file_reader_agent import FileReaderAgent
from memory.memory_manager import MemoryManager
from memory.memory_item import (
    AgentType, RawContextItem, SubTaskItem, BaselineContextItem, MemoryType
)
from prompts.loader import get_prompt_loader
from utils.text_utils import deduplicate_text


class ProbeAgent(BaseAgent):
    """æ¢æµ‹å™¨æ™ºèƒ½ä½“ - åªè¯»ç³»ç»ŸçŠ¶æ€æ•°æ®æ”¶é›†"""

    def __init__(self,
                 llm_config: AgentConfig,
                 memory_manager: MemoryManager,
                 max_iterations: int = 5,
                 task_description: str = "",
                 available_actions: Dict[str, str] = None,
                 api_instruction: str = ""):
        """
        åˆå§‹åŒ–æ¢æµ‹å™¨æ™ºèƒ½ä½“

        Args:
            llm_config: LLMé…ç½®
            memory_manager: å†…å­˜ç®¡ç†å™¨
            max_iterations: æœ€å¤§æ¢æµ‹è½®æ¬¡
            task_description: ä»»åŠ¡æè¿°
            available_actions: å¯ç”¨çš„APIåŠ¨ä½œ
            api_instruction: APIä½¿ç”¨è¯´æ˜
        """
        self.task_description = task_description
        self.available_actions = available_actions or {}
        self.api_instruction = api_instruction or ""  # æ”¹å
        self.prompt_loader = get_prompt_loader()

        super().__init__(
            name="Probe Agent",
            agent_type=AgentType.PROBE,
            llm_config=llm_config,
            memory_manager=memory_manager,
            max_iterations=max_iterations
        )

        # agent_logger å·²åœ¨åŸºç±»ä¸­åˆå§‹åŒ–
        self.probe_results = []
        self.probe_round = 0
        self.successful_commands = []  # æˆåŠŸçš„å‘½ä»¤å†å²
        self.failed_commands = []  # å¤±è´¥çš„å‘½ä»¤å†å²
        self.file_reader = FileReaderAgent(llm_config)  # æ–‡ä»¶è¯»å–agent
        self.baseline_context = ""  # å‰ä¸¤ä¸ªiterçš„åŸºç¡€ä¿¡æ¯ï¼ˆå‘½åç©ºé—´ã€æœåŠ¡åˆ—è¡¨ç­‰ï¼‰

    def _initialize_llm_agent(self) -> Agent:
        """åˆå§‹åŒ–LLMæ™ºèƒ½ä½“"""
        return Agent(
            name=self.name,
            conf=self.llm_config,
            system_prompt=self._get_system_prompt()
        )

    def _get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        # ç°åœ¨éœ€è¦ä¼ å…¥available_actionså’Œapi_instruction
        return self.prompt_loader.get_prompt(
            agent_type="probe",
            prompt_type="system",
            available_actions=self._format_available_actions(),
            api_instruction=self.api_instruction
        )

    def _classify_result(self, result: str) -> bool:
        """
        åˆ†ç±»æ‰§è¡Œç»“æœæ˜¯å¦æˆåŠŸ

        Args:
            result: æ‰§è¡Œç»“æœå­—ç¬¦ä¸²

        Returns:
            True if successful, False if error
        """
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆå¦‚æœæ˜¯dictï¼‰
        if isinstance(result, dict):
            result = json.dumps(result)

        # æ£€æŸ¥æ˜¯å¦åŒ…å«é”™è¯¯æ ‡è®°
        error_patterns = [
            r'^Error',  # ä»¥Errorå¼€å¤´
            r'Error from server',  # Kubernetes API é”™è¯¯
            r'Syntax error',
            # r'^No \w+ found',
            # r'Error parsing response',  # è§£æé”™è¯¯
            # r'No API call found',  # APIè°ƒç”¨æœªæ‰¾åˆ°
            # r'command not found',  # å‘½ä»¤æœªæ‰¾åˆ°
            # r'permission denied',  # æƒé™æ‹’ç»
            # r'no such file or directory',  # æ–‡ä»¶ä¸å­˜åœ¨
            # r'cannot access',  # æ— æ³•è®¿é—®
            # r'failed to',  # å¤±è´¥
            # r'unable to'  # æ— æ³•
        ]

        for pattern in error_patterns:
            if re.search(pattern, result, re.IGNORECASE):
                return False

        return True

    def _prepare_input(self, task_instruction, context: Dict[str, Any],
                       **kwargs) -> str:  # æ”¹åï¼šinstruction -> task_instruction
        """å‡†å¤‡LLMè¾“å…¥"""
        probe_round = kwargs.get("probe_round", self.probe_round)
        successful_history = kwargs.get("successful_probe_history", self.successful_commands)
        failed_history = kwargs.get("failed_probe_history", self.failed_commands)
        probe_context = kwargs.get("probe_context", "")
        current_subtask = kwargs.get("current_subtask", None)
        
        # è·å–å½“å‰ iteration ç¼–å·
        current_iteration = current_subtask.iteration_number if current_subtask else 1

        # å¤„ç† baseline_context çš„æ¡ä»¶æ˜¾ç¤ºï¼ˆä¸å†æˆªæ–­ï¼Œæä¾›å®Œæ•´ä¿¡æ¯ï¼‰
        if self.baseline_context:
            # ç›´æ¥ä½¿ç”¨å®Œæ•´çš„baseline_contextï¼Œä¸è¿›è¡Œæˆªæ–­
            # LLMåº”è¯¥èƒ½å¤Ÿå¤„ç†å®Œæ•´çš„å†å²ä¿¡æ¯ä»¥åšå‡ºå‡†ç¡®å†³ç­–
            baseline_display = self.baseline_context
        else:
            baseline_display = "No baseline yet. Collect: namespaces, services, pods."
        
        # å°†JSONæ ¼å¼çš„probe_contextè½¬æ¢ä¸ºMarkdownæ ¼å¼
        probe_context_display = self._format_probe_context_to_markdown(probe_context)
        
        # ä¸æˆªæ–­probe_contextï¼Œä¿ç•™å®Œæ•´ä¿¡æ¯ä¾›æ¢æµ‹å™¨ä½¿ç”¨
        
        # ä»probe_contextä¸­æå–å…³é”®å­—æ®µï¼ˆç”¨äºç®€çŸ­æç¤ºï¼‰
        investigation_phase = "surface_scan"
        investigation_type = "health_check"
        primary_targets = []
        suggested_commands = []
        
        if isinstance(probe_context, dict):
            investigation_phase = probe_context.get('investigation_phase', 'surface_scan')
            
            investigation_focus = probe_context.get('investigation_focus', {})
            investigation_type = investigation_focus.get('investigation_type', 'health_check')
            primary_targets = investigation_focus.get('primary_targets', [])
            # å›æ»šï¼šä¸å†è¯»å– suggested_commands
        
        # å›æ»šï¼šä¸å†å±•ç¤º suggested_commands
        
        params = {
            "task_instruction": task_instruction,  # æ”¹å
            "successful_probe_history": self._format_probe_history(successful_history, success=True, current_round=probe_round),
            "failed_probe_history": self._format_probe_history(failed_history, success=False, current_round=probe_round),
            "baseline_context": baseline_display,  # å‰ä¸¤ä¸ªiterçš„åŸºç¡€ä¿¡æ¯
            "probe_context": probe_context_display,
            "probe_round": probe_round,
            "max_iterations": self.max_iterations,
            # é¢å¤–çš„å…³é”®å­—æ®µï¼ˆç”¨äºç®€çŸ­æç¤ºï¼‰
            "investigation_phase": investigation_phase,
            "investigation_type": investigation_type,
            "primary_targets": ", ".join(primary_targets) if primary_targets else "Not specified"
        }

        return self.prompt_loader.get_prompt(
            agent_type="probe",
            prompt_type="user",
            **params
        )

    def _format_available_actions(self) -> str:
        """æ ¼å¼åŒ–å¯ç”¨åŠ¨ä½œ"""
        if not self.available_actions:
            return "No specific actions defined - use standard kubectl commands"

        formatted = []
        for action_name, action_desc in self.available_actions.items():
            # ä¿ç•™å®Œæ•´çš„APIæ–‡æ¡£
            formatted.append(f"**{action_name}**: {action_desc}")

        return "\n\n".join(formatted)

    def _format_probe_history(self, probe_history, success: bool = True, current_round: int = None) -> str:
        """
        æ ¼å¼åŒ–æ¢æµ‹å†å²
        - æˆåŠŸå‘½ä»¤ï¼šæ˜¾ç¤ºæ‰€æœ‰æˆåŠŸå‘½ä»¤ï¼ˆä»round 1å¼€å§‹ï¼‰ï¼Œåªæ˜¾ç¤ºå‘½ä»¤ä¸æ˜¾ç¤ºç»“æœ
        - å¤±è´¥å‘½ä»¤ï¼šåªæ˜¾ç¤ºæœ€åä¸€ä¸ªçš„å‘½ä»¤+é”™è¯¯ï¼Œå…¶ä»–åªæ˜¾ç¤ºå‘½ä»¤
        """
        if not probe_history:
            return f"No {'successful' if success else 'failed'} commands yet."

        history_type = "âœ“ SUCCESSFUL" if success else "âœ— FAILED"

        # æ ¹æ®æˆåŠŸ/å¤±è´¥æ˜¾ç¤ºä¸åŒæ•°é‡çš„å†å²
        display_count = 10 if success else 5

        # ä¸å†è¿‡æ»¤ï¼Œæ˜¾ç¤ºæ‰€æœ‰å†å²ï¼ˆä»round 1å¼€å§‹ï¼‰
        filtered_history = probe_history

        if not filtered_history:
            return f"No {'successful' if success else 'failed'} commands yet."

        # è·å–æœ€è¿‘çš„å‘½ä»¤
        recent_items = filtered_history[-display_count:]
        
        formatted_commands = []
        
        for i, item in enumerate(recent_items):
            if isinstance(item, dict):
                command = item.get("command", "")
                result = item.get("result", "")
                item_round = item.get("round", 0)
            else:
                # å…¼å®¹æ—§æ ¼å¼ï¼ˆå­—ç¬¦ä¸²ï¼‰
                command = item
                result = ""
                item_round = 0
            
            if success:
                # æˆåŠŸå‘½ä»¤ï¼šåªæ˜¾ç¤ºå‘½ä»¤ï¼Œä¸æ˜¾ç¤ºç»“æœ
                formatted_commands.append(f"  - Round {item_round}: {command}")
            else:
                # å¤±è´¥å‘½ä»¤ï¼šåªæœ‰æœ€åä¸€ä¸ªæ˜¾ç¤ºå®Œæ•´é”™è¯¯ï¼Œå…¶ä»–åªæ˜¾ç¤ºå‘½ä»¤
                if i == len(recent_items) - 1 and result:
                    # ä¸æˆªæ–­ï¼Œæ˜¾ç¤ºå®Œæ•´é”™è¯¯ä¿¡æ¯
                    formatted_commands.append(f"  - {command}\n    âŒ Error: {result}")
                else:
                    formatted_commands.append(f"  - {command}")

        # å¦‚æœå†å²è¶…è¿‡æ˜¾ç¤ºæ•°é‡ï¼Œæ·»åŠ æç¤º
        header = ""
        if len(filtered_history) > display_count:
            header = f"[{history_type}] Showing last {display_count} of {len(filtered_history)} commands:\n"
        else:
            header = f"[{history_type}] {len(filtered_history)} commands:\n"
        
        return header + "\n".join(formatted_commands)

    def _format_probe_context_to_markdown(self, probe_context) -> str:
        """
        å°†JSONæ ¼å¼çš„probe_contextè½¬æ¢ä¸ºæ¸…æ™°çš„Markdownæ ¼å¼
        
        Args:
            probe_context: Observeræä¾›çš„è°ƒæŸ¥æŒ‡å¯¼ä¿¡æ¯ï¼Œå¯èƒ½æ˜¯dictæˆ–str
            
        Returns:
            str: æ ¼å¼åŒ–çš„Markdownæ–‡æœ¬
        """
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºJSON
        if isinstance(probe_context, str):
            if not probe_context or probe_context.strip() == "":
                return "No investigation context provided"
            # å°è¯•è§£æJSON
            try:
                probe_context = json.loads(probe_context)
            except:
                # å¦‚æœä¸æ˜¯JSONï¼Œç›´æ¥è¿”å›åŸæ–‡æœ¬ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
                return probe_context
        
        # å¦‚æœä¸æ˜¯dictï¼Œè¿”å›ç©º
        if not isinstance(probe_context, dict):
            return str(probe_context)
        
        md_parts = []
        
        # 1. è°ƒæŸ¥é˜¶æ®µ
        investigation_phase = probe_context.get('investigation_phase', 'Not specified')
        phase_emoji = {
            'surface_scan': 'ğŸ”',
            'point_investigation': 'ğŸ¯',
            'depth_analysis': 'ğŸ”¬',
            'verification': 'âœ…'
        }
        emoji = phase_emoji.get(investigation_phase, 'ğŸ“‹')
        md_parts.append(f"### {emoji} Investigation Phase\n**{investigation_phase.replace('_', ' ').title()}**\n")
        
        # 2. è°ƒæŸ¥ç„¦ç‚¹
        investigation_focus = probe_context.get('investigation_focus', {})
        if investigation_focus:
            md_parts.append("### ğŸ¯ Investigation Focus\n")
            
            primary_targets = investigation_focus.get('primary_targets', [])
            if primary_targets:
                md_parts.append(f"**Primary Targets**: {', '.join([f'`{t}`' for t in primary_targets])}\n")
            
            investigation_type = investigation_focus.get('investigation_type', '')
            if investigation_type:
                md_parts.append(f"**Investigation Type**: {investigation_type}\n")
            
            specific_checks = investigation_focus.get('specific_checks', [])
            if specific_checks:
                md_parts.append("\n**Specific Checks**:")
                for check in specific_checks:
                    md_parts.append(f"- {check}")
                md_parts.append("")  # ç©ºè¡Œ
            
            # å›æ»šï¼šä¸å†å±•ç¤º suggested_commands
        
        # 3. æ¯”è¾ƒåˆ†æè¦æ±‚
        comparison_requirements = probe_context.get('comparison_requirements', {})
        if comparison_requirements:
            need_comparison = comparison_requirements.get('need_comparison', False)
            if need_comparison:
                md_parts.append("### ğŸ“Š Comparison Requirements\n")
                
                comparison_targets = comparison_requirements.get('comparison_targets', [])
                if comparison_targets:
                    md_parts.append("**Comparison Targets**:")
                    for target in comparison_targets:
                        md_parts.append(f"- {target}")
                    md_parts.append("")
                
                outlier_detection = comparison_requirements.get('outlier_detection', '')
                if outlier_detection:
                    md_parts.append(f"**Outlier Detection**: {outlier_detection}\n")
        
        return "\n".join(md_parts)

    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """è§£æLLMå“åº”ä¸ºç»“æ„åŒ–æ•°æ®"""
        parsed = {
            "raw_response": response,
            "probe_command": None,
            "next_action": "CONTINUE",
            "reasoning": "",
            "focus_area": "",
            "analysis": ""
        }

        try:
            # å°è¯•JSONè§£æ
            json_match = re.search(r'\{.*}', response, re.DOTALL)
            if json_match:
                json_data = json.loads(json_match.group())

                # æå–æ‰€æœ‰å­—æ®µ
                for key in ["probe_command", "next_action", "reasoning",
                            "focus_area", "analysis"]:
                    if key in json_data:
                        parsed[key] = json_data[key]

        except Exception as e:
            self.logger.error(f"Failed to parse LLM response: {e}")

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å‘½ä»¤ï¼Œå°è¯•å…¶ä»–æ–¹å¼æå–
        if not parsed["probe_command"]:
            # æŸ¥æ‰¾å¯èƒ½çš„å‘½ä»¤æ¨¡å¼
            command_patterns = [
                r'"probe_command"\s*:\s*"([^"]+)"',
                r'`([^`]+)`',  # ä»£ç å—ä¸­çš„å‘½ä»¤
                r'kubectl\s+[^\n]+',  # kubectlå‘½ä»¤
                r'exec_shell\(["\']([^"\']+)["\']\)',  # exec_shellæ ¼å¼
            ]

            for pattern in command_patterns:
                match = re.search(pattern, response)
                if match:
                    parsed["probe_command"] = match.group(1) if '(' in pattern else match.group(0)
                    break

        return parsed

    def _process_decision(self,
                          decision: Dict[str, Any],
                          context: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å†³ç­–ç»“æœ"""
        self.probe_round += 1

        # è·å–æ¢æµ‹å‘½ä»¤å’Œå…¶ä»–ä¿¡æ¯
        probe_command = decision.get("probe_command", "")
        next_action = decision.get("next_action", "CONTINUE")
        reasoning = decision.get("reasoning", "")
        focus_area = decision.get("focus_area", "")
        analysis = decision.get("analysis", "")

        if not probe_command:
            self.agent_logger.error("âŒ No probe command found")
            return {
                "probe_command": "",
                "next_action": "COMPLETE",
                "error": "No probe command found",
                "round": self.probe_round
            }

        # è¾“å‡ºæ¢æµ‹ä¿¡æ¯
        self.agent_logger.info(f"ğŸ” Round {self.probe_round}: {probe_command[:80]}...")
        if focus_area:
            self.agent_logger.info(f"ğŸ¯ Focus: {focus_area}")

        # æ„å»ºç»“æœ
        result = {
            "probe_command": probe_command,
            "next_action": next_action,
            "reasoning": reasoning,
            "focus_area": focus_area,
            "analysis": analysis,
            "round": self.probe_round,
            "timestamp": datetime.now().isoformat()
        }

        self.probe_results.append(result)
        return result

    def _get_fallback_decision(self) -> Dict[str, Any]:
        """è·å–å¤‡ç”¨å†³ç­–"""
        return {
            "probe_command": "",
            "next_action": "COMPLETE",
            "error": "Failed to get valid decision from LLM",
            "round": self.probe_round
        }

    async def probe_system(self,
                           task_instruction,  # æ”¹åï¼šinstruction -> task_instruction
                           current_subtask: Optional[SubTaskItem] = None,
                           successful_probe_history: List[Dict] = None,
                           failed_probe_history: List[Dict] = None,
                           probe_round: int = None,
                           probe_context: str = "") -> Dict[str, Any]:
        """
        æ‰§è¡Œç³»ç»Ÿæ¢æµ‹ - ä¸»è¦æ¥å£

        Args:
            task_instruction: å­ä»»åŠ¡æŒ‡å¼•æ¥è‡ªè§‚å¯Ÿå™¨
            current_subtask: å½“å‰å­ä»»åŠ¡
            successful_probe_history: æˆåŠŸçš„æ¢æµ‹å†å²
            failed_probe_history: å¤±è´¥çš„æ¢æµ‹å†å²
            probe_round: å½“å‰æ¢æµ‹è½®æ¬¡
            probe_context: æ¢æµ‹å™¨ä¸Šä¸‹æ–‡ï¼ˆæ¥è‡ªObserverï¼‰
        """
        try:
            if probe_round is not None:
                self.probe_round = probe_round - 1  # å› ä¸º_process_decisionä¼š+1

            result = await self.process(
                task_instruction,
                context={},
                current_subtask=current_subtask,
                probe_round=probe_round or (self.probe_round + 1),
                successful_probe_history=successful_probe_history or [],
                failed_probe_history=failed_probe_history or [],
                probe_context=probe_context,
            )

            if isinstance(result, dict):
                return result
            else:
                self.logger.error(f"Unexpected result type: {type(result)}")
                return self._get_fallback_decision()

        except Exception as e:
            self.logger.error(f"Error in probe_system: {e}")
            return self._get_fallback_decision()

    async def probe_run(self,
                        task_instruction: str,
                        execute_action: Callable[[str], str],
                        current_subtask: Optional[SubTaskItem] = None,
                        session_id: Optional[str] = None,
                        probe_context: str = "") -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„å¤šè½®æ¢æµ‹æµç¨‹ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        Args:
            task_instruction: å­ä»»åŠ¡æŒ‡å¼•æ¥è‡ªè§‚å¯Ÿå™¨
            execute_action: æ‰§è¡Œå‘½ä»¤çš„å‡½æ•°
            current_subtask: å½“å‰å­ä»»åŠ¡
            session_id: ä¼šè¯ID
            probe_context: æ¢æµ‹å™¨ä¸Šä¸‹æ–‡ï¼ˆæ¥è‡ªObserverï¼‰
        """
        # é‡ç½®çŠ¶æ€
        self.reset()
        if session_id:
            self.session_id = session_id
        
        # ä» current_subtask è·å– iteration
        current_iteration = current_subtask.iteration_number if current_subtask else 1

        # åˆå§‹åŒ–
        all_results = []
        retry_count = 0  # å½“å‰è½®æ¬¡çš„é‡è¯•æ¬¡æ•°
        max_retries_per_round = 3  # æ¯è½®æœ€å¤šé‡è¯•æ¬¡æ•°

        self.agent_logger.info(f"ğŸš€ Starting probe (max {self.max_iterations} rounds)")

        probe_round = 1
        while probe_round <= self.max_iterations:
            try:
                # è°ƒç”¨æ¢æµ‹ç³»ç»Ÿ
                probe_result = await self.probe_system(
                    task_instruction=task_instruction,
                    current_subtask=current_subtask,
                    successful_probe_history=self.successful_commands,
                    failed_probe_history=self.failed_commands,
                    probe_round=probe_round,
                    probe_context=probe_context,
                )

                # æå–å‘½ä»¤å’Œä¸‹ä¸€æ­¥åŠ¨ä½œ
                command = probe_result.get("probe_command", "")
                next_action = probe_result.get("next_action", "CONTINUE")
                focus_area = probe_result.get("focus_area", "")
                analysis = probe_result.get("analysis", "")

                # æ‰§è¡Œå‘½ä»¤
                exec_result = ""
                is_success = True
                retry_used = False

                if command:
                    try:
                        exec_result = execute_action(command)

                        # ç«‹å³å¯¹ç»“æœè¿›è¡Œå»é‡å¤„ç†
                        if exec_result and isinstance(exec_result, str) and len(exec_result) > 1000:
                            deduplicated_result, stats = deduplicate_text(exec_result)
                            if stats["reduction_ratio"] > 0.1:  # åªæœ‰å»é‡æ•ˆæœè¶…è¿‡10%æ‰è®°å½•å’Œä½¿ç”¨
                                self.agent_logger.info(
                                    f"  ğŸ“ Deduplication: {stats['original_length']} â†’ {stats['deduplicated_length']} chars "
                                    f"(reduced {stats['reduction_ratio']:.1%})"
                                )
                                exec_result = deduplicated_result
                        
                        # æ£€æµ‹å¹¶å¤„ç†CSVæ–‡ä»¶ï¼ˆget_traces/get_metricsç­‰ï¼‰
                        if self.file_reader.should_read_files(command):
                            enhanced_result, read_files = await self.file_reader.process_result(
                                command=command,
                                result_text=exec_result,
                                task_instruction=task_instruction
                            )
                            if read_files:
                                exec_result = enhanced_result  # ä½¿ç”¨å¢å¼ºåçš„ç»“æœ
                        
                        is_success = self._classify_result(exec_result)

                        # å¾ªç¯é‡è¯•ï¼Œç›´åˆ°æˆåŠŸæˆ–è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                        while not is_success and retry_count < max_retries_per_round:
                            self.agent_logger.warning(
                                f"âš ï¸ Command failed, attempting retry {retry_count + 1}/{max_retries_per_round}")

                            # è®°å½•å¤±è´¥çš„å‘½ä»¤ï¼ˆåŒ…å«é”™è¯¯ä¿¡æ¯ï¼Œç”¨äºt-1æ˜¾ç¤ºï¼‰
                            self.failed_commands.append({"command": command, "result": exec_result})

                            retry_count += 1
                            retry_used = True

                            # ç«‹å³é‡æ–°ç”Ÿæˆå¹¶æ‰§è¡Œæ¢æµ‹å‘½ä»¤
                            self.agent_logger.info(f"ğŸ”„ Retrying with adjusted approach...")

                            # é‡æ–°è°ƒç”¨probe_systemç”Ÿæˆæ–°å‘½ä»¤ï¼ˆä¼šè€ƒè™‘å¤±è´¥å†å²ï¼‰
                            # æä¾›å®Œæ•´çš„é”™è¯¯ä¿¡æ¯ï¼Œä¸è¿›è¡Œæˆªæ–­ï¼Œä»¥ä¾¿LLMèƒ½å¤Ÿå‡†ç¡®åˆ†æå¤±è´¥åŸå› 
                            retry_result = await self.probe_system(
                                task_instruction=task_instruction + f"\n[RETRY HINT] Previous command '{command}' failed with: {exec_result}",
                                current_subtask=current_subtask,
                                successful_probe_history=self.successful_commands,
                                failed_probe_history=self.failed_commands,
                                probe_round=probe_round,
                                probe_context=probe_context,
                            )

                            retry_command = retry_result.get("probe_command", "")
                            if retry_command and retry_command != command:  # ç¡®ä¿ä¸æ˜¯ç›¸åŒçš„å‘½ä»¤
                                exec_result = execute_action(retry_command)
                                is_success = self._classify_result(exec_result)
                                command = retry_command  # æ›´æ–°å‘½ä»¤è®°å½•

                                if is_success:
                                    self.agent_logger.success(f"âœ… Retry successful!")
                            else:
                                self.agent_logger.warning(f"âš ï¸ Could not generate alternative command, skipping retry")
                                break  # æ— æ³•ç”Ÿæˆæ–°å‘½ä»¤ï¼Œåœæ­¢é‡è¯•

                        # æ ¹æ®æœ€ç»ˆç»“æœåˆ†ç±»å­˜å‚¨
                        command_record = {
                            "round": probe_round,
                            "command": command,
                            "result": exec_result,
                            "focus_area": focus_area,
                            "analysis": analysis,
                            "retry_used": retry_used
                        }

                        if is_success:
                            # è®°å½•æˆåŠŸçš„å‘½ä»¤ï¼ˆåªä¿å­˜å‘½ä»¤ã€roundå’Œiterationï¼Œä¸ä¿å­˜ç»“æœï¼‰
                            self.successful_commands.append({
                                "command": command, 
                                "round": probe_round,
                                "iteration": current_iteration
                            })
                            self.agent_logger.info(f"âœ… Command successful{' (after retry)' if retry_used else ''}")

                            # åªæœ‰æˆåŠŸçš„ç»“æœæ‰ä¿å­˜åˆ°Memory
                            if self.memory_manager:
                                raw_item = RawContextItem(
                                    source_agent=self.agent_type,
                                    source_agent_id=self.session_id or "",
                                    round_number=probe_round,
                                    raw_output=exec_result,
                                    command=command,
                                    execution_time=0.0,
                                    success=True,
                                    metadata={
                                        "session_id": self.session_id,
                                        "source_agent": self.agent_type.value,
                                        "iteration": current_iteration,
                                        "round_number": probe_round,
                                        "command": command,
                                        "focus_area": focus_area,
                                        "result": exec_result,
                                        "retry_used": retry_used
                                    }
                                )

                                self.memory_manager.add_item(raw_item, self.agent_type)

                            # æˆåŠŸåé‡ç½®é‡è¯•è®¡æ•°
                            retry_count = 0
                            
                            # ä¿å­˜å‰ä¸¤ä¸ªITERATIONçš„æ‰€æœ‰æˆåŠŸç»“æœä½œä¸ºbaseline context
                            if current_iteration <= 2:
                                # æ ¼å¼åŒ–ä¿å­˜ï¼šå‘½ä»¤ + å®Œæ•´ç»“æœï¼ˆä¸æˆªæ–­ï¼‰
                                baseline_entry = f"\n### Iteration {current_iteration}, Round {probe_round}\n**Command**: {command}\n**Result**:\n{exec_result}\n"
                                self.baseline_context += baseline_entry
                                self.agent_logger.info(f"ğŸ“ Saved to baseline context (Iter {current_iteration}, Round {probe_round})")
                        else:
                            # è®°å½•å¤±è´¥çš„å‘½ä»¤ï¼ˆåŒ…å«é”™è¯¯ä¿¡æ¯ï¼Œç”¨äºt-1æ˜¾ç¤ºï¼‰
                            # é¿å…é‡å¤è®°å½•ï¼šæ£€æŸ¥æœ€åä¸€æ¡è®°å½•æ˜¯å¦å·²ç»æ˜¯å½“å‰å‘½ä»¤
                            if not self.failed_commands or self.failed_commands[-1]["command"] != command:
                                self.failed_commands.append({"command": command, "result": exec_result})
                            self.agent_logger.warning(f"âš ï¸ Command failed after all retries")
                            # æå–é”™è¯¯ä¿¡æ¯æ˜¾ç¤º
                            error_match = re.search(r'Error:\s*(.+?)(?:\n|$)', exec_result)
                            if error_match:
                                self.agent_logger.error(f"Error: {error_match.group(1)}")

                        # æ˜¾ç¤ºç»“æœé¢„è§ˆ
                        result_preview = exec_result[:200] + "..." if len(exec_result) > 200 else exec_result
                        self.agent_logger.info(f"ğŸ“Š Result preview: {result_preview}")

                    except Exception as e:
                        self.agent_logger.error(f"âŒ Execution error: {e}")

                        # å°è¯•é‡è¯•
                        if retry_count < max_retries_per_round:
                            retry_count += 1
                            self.agent_logger.warning(
                                f"âš ï¸ Exception occurred, attempting retry {retry_count}/{max_retries_per_round}")

                            # è®°å½•å¤±è´¥ï¼ˆåŒ…å«é”™è¯¯ä¿¡æ¯ï¼Œç”¨äºt-1æ˜¾ç¤ºï¼‰
                            self.failed_commands.append({"command": command, "result": f"Error: {str(e)}"})

                            # è·³è¿‡å½“å‰è½®æ¬¡ï¼Œè®©ä¸‹ä¸€è½®å°è¯•ä¸åŒçš„æ–¹æ³•
                            probe_round += 1
                            retry_count = 0
                            continue

                        exec_result = f"Error: {str(e)}"
                        is_success = False

                        # è®°å½•å¤±è´¥ï¼ˆåŒ…å«é”™è¯¯ä¿¡æ¯ï¼Œç”¨äºt-1æ˜¾ç¤ºï¼‰
                        self.failed_commands.append({"command": command, "result": exec_result})

                # è®°å½•ç»“æœ
                round_info = {
                    "round": probe_round,
                    "command": command,
                    "result": exec_result if exec_result else "",  # ä¸æˆªæ–­ç»“æœ
                    "success": is_success,
                    "next_action": next_action,
                    "focus_area": focus_area,
                    "timestamp": datetime.now().isoformat(),
                    "retry_used": retry_used
                }
                all_results.append(round_info)

                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                if next_action == "COMPLETE":
                    self.agent_logger.success(f"âœ¨ Probe completed at round {probe_round}")
                    break

                # å‰è¿›åˆ°ä¸‹ä¸€è½®
                probe_round += 1
                retry_count = 0  # é‡ç½®é‡è¯•è®¡æ•°

                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è½®æ¬¡
                if probe_round > self.max_iterations:
                    self.agent_logger.warning(f"â±ï¸ Reached maximum iterations ({self.max_iterations})")
                    break

            except Exception as e:
                self.agent_logger.error(f"ğŸ’¥ Error in probe round {probe_round}: {e}")
                error_info = {
                    "round": probe_round,
                    "error": str(e),
                    "next_action": "ERROR",
                    "timestamp": datetime.now().isoformat()
                }
                all_results.append(error_info)
                break

        # åªè¿”å›æˆåŠŸçš„å‘½ä»¤å†å²ä½œä¸ºprobe_historyï¼ˆä¸æˆªæ–­ï¼‰
        successful_history = "\n".join([
            f"Round {item['round']}: {item['command']}\nResult: {item.get('result', '')}"
            for item in all_results if item.get('success', False)
        ])

        # ä¿å­˜baseline_contextåˆ°memoryï¼ˆå¦‚æœæœ‰ä¸”å½“å‰iteration <= 2ï¼‰
        if self.baseline_context and current_iteration <= 2 and self.memory_manager and session_id:
            baseline_item = BaselineContextItem(
                session_id=session_id,
                iteration_numbers=[i for i in range(1, current_iteration + 1)],
                baseline_content=self.baseline_context,
                commands_included=[cmd["command"] for cmd in self.successful_commands if cmd.get("iteration", 0) <= 2],
                metadata={
                    "session_id": session_id,
                    "created_at": datetime.now().isoformat(),
                    "source_agent": "probe"
                }
            )
            self.memory_manager.add_item(baseline_item, self.agent_type)
            self.agent_logger.info(f"ğŸ’¾ Baseline context saved to memory for iterations {baseline_item.iteration_numbers}")

        # è¿”å›å®Œæ•´ç»“æœ
        return {
            "total_rounds": len(all_results),
            "completed": True,
            "results": all_results,
            "probe_history": successful_history,
            "successful_commands_list": self.successful_commands,  # æ·»åŠ å®Œæ•´çš„æˆåŠŸå‘½ä»¤åˆ—è¡¨
            "failed_commands_list": self.failed_commands,  # æ·»åŠ å®Œæ•´çš„å¤±è´¥å‘½ä»¤åˆ—è¡¨
            "successful_commands": len(self.successful_commands),
            "failed_commands": len(self.failed_commands),
            "final_status": all_results[-1]["next_action"] if all_results else "NO_RESULTS",
            "session_id": self.session_id,
            "retries_used": sum(1 for r in all_results if r.get("retry_used", False)),
            "baseline_context": self.baseline_context  # è¿”å›baseline_context
        }

    def reset(self):
        """é‡ç½®æ¢æµ‹å™¨çŠ¶æ€"""
        super().reset()
        self.probe_results = []
        self.probe_round = 0
        # ä¸å†é‡ç½®ä»¥ä¸‹å†…å®¹ï¼Œè®©å®ƒä»¬åœ¨æ•´ä¸ªä»»åŠ¡æœŸé—´ç´¯ç§¯ï¼š
        # - self.successful_commands
        # - self.failed_commands
        # - self.baseline_context (å‰ä¸¤ä¸ª iteration çš„åŸºç¡€ä¿¡æ¯)