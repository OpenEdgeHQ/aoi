from typing import Dict, Any, Optional, Callable, List
from datetime import datetime
import json
import re

from aworld.agents.llm_agent import Agent
from aworld.config.conf import AgentConfig

from agents.base_agent import BaseAgent
from agents.probe_agent import ProbeAgent
from agents.file_reader_agent import FileReaderAgent
from memory.memory_manager import MemoryManager
from memory.memory_item import (
    AgentType, SubTaskItem, RawContextItem, BaselineContextItem, MemoryType
)
from prompts.loader import get_prompt_loader
from utils.text_utils import deduplicate_text


class ExecutorAgent(BaseAgent):
    """æ‰§è¡Œå™¨æ™ºèƒ½ä½“ - è´Ÿè´£ä¿®å¤æ•…éšœ"""

    def __init__(self,
                 llm_config: AgentConfig,
                 memory_manager: MemoryManager,
                 probe_agent: Optional[ProbeAgent] = None,
                 max_iterations: int = 5,
                 use_probe: bool = False,
                 task_description: str = "",
                 available_actions: Dict[str, str] = None,
                 api_instruction: str = ""):  # æ”¹åï¼šinstructions -> api_instruction
        """
        åˆå§‹åŒ–æ‰§è¡Œå™¨æ™ºèƒ½ä½“

        Args:
            llm_config: LLMé…ç½®
            memory_manager: å†…å­˜ç®¡ç†å™¨
            probe_agent: æ¢æµ‹å™¨æ™ºèƒ½ä½“å®ä¾‹ï¼ˆå¯é€‰ï¼‰
            max_iterations: æœ€å¤§æ‰§è¡Œè½®æ¬¡
            use_probe: æ˜¯å¦å…è®¸LLMè‡ªä¸»å†³å®šä½¿ç”¨æ¢æµ‹å™¨
            task_description: ä»»åŠ¡æè¿°
            available_actions: å¯ç”¨çš„APIåŠ¨ä½œ
            api_instruction: APIä½¿ç”¨è¯´æ˜æ ¼å¼
        """
        self.task_description = task_description
        self.available_actions = available_actions or {}
        self.api_instruction = api_instruction or ""  # æ”¹å
        self.use_probe = use_probe

        # åˆå§‹åŒ–æˆ–åˆ›å»ºæ¢æµ‹å™¨
        if not probe_agent and use_probe:
            self.probe_agent = ProbeAgent(
                llm_config=llm_config,
                memory_manager=memory_manager,
                max_iterations=1,  # å•æ¬¡æ¢æµ‹
                task_description=task_description,
                available_actions=available_actions,
                api_instruction=api_instruction
            )
        else:
            self.probe_agent = probe_agent

        self.prompt_loader = get_prompt_loader()

        super().__init__(
            name="Executor Agent",
            agent_type=AgentType.EXECUTOR,
            llm_config=llm_config,
            memory_manager=memory_manager,
            max_iterations=max_iterations
        )

        # agent_logger å·²åœ¨åŸºç±»ä¸­åˆå§‹åŒ–
        self.execution_results = []
        self.execution_round = 0
        self.successful_commands = []  # æˆåŠŸçš„å‘½ä»¤å†å²
        self.failed_commands = []  # å¤±è´¥çš„å‘½ä»¤å†å²
        self.failure_summaries = []  # å¤±è´¥æ€»ç»“ç´¯ç§¯åˆ—è¡¨ï¼ˆæ–°å¢ï¼‰
        self.file_reader = FileReaderAgent(llm_config)  # æ–‡ä»¶è¯»å–agent
        self.baseline_context = ""  # ä»æ¢æµ‹å™¨è·å–çš„åŸºçº¿ä¸Šä¸‹æ–‡

    def _initialize_llm_agent(self) -> Agent:
        """åˆå§‹åŒ–LLMæ™ºèƒ½ä½“"""
        return Agent(
            name=self.name,
            conf=self.llm_config,
            system_prompt=self._get_system_prompt()
        )

    def _get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯ - æ‰§è¡Œä¿®å¤ç‰ˆæœ¬"""
        # æ ¹æ®æ˜¯å¦å¯ç”¨æ¢æµ‹å™¨ï¼Œé€‰æ‹©ä¸åŒçš„probe_section
        if self.use_probe:
            probe_section = """
## Probe Usage (Enabled)
You can decide to probe the system for more information before executing repair:
- Set "use_probe": true and provide "probe_instruction" when you need more data
- Set "use_probe": false when you have enough information to proceed
- Probe is useful when: unclear error states, need to verify system status, complex failures
"""
        else:
            probe_section = """
## Probe Usage (Disabled)
Direct probe access is not available. You must proceed with the information provided.
Focus on making targeted fixes based on the diagnosis from the Observer.
"""

        return self.prompt_loader.get_prompt(
            agent_type="executor",
            prompt_type="system",
            probe_section=probe_section,
            available_actions=self._format_available_actions(),
            api_instruction=self.api_instruction
        )

    def _classify_result(self, result: str) -> bool:
        """
        åˆ†ç±»æ‰§è¡Œç»“æœæ˜¯å¦æˆåŠŸ
        Args:
            result: æ‰§è¡Œç»“æœå­—ç¬¦ä¸²

        Returns:
            True if successful, False if error
        """
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆå¦‚æœæ˜¯dictï¼‰
        if isinstance(result, dict):
            result = json.dumps(result)

        # æ£€æŸ¥æ˜¯å¦åŒ…å«é”™è¯¯æ ‡è®°
        error_patterns = [
            r'^Error:',  # ä»¥Error:å¼€å¤´
            r'^[ERROR]'
            r'Error from server',  # KubernetesæœåŠ¡å™¨é”™è¯¯
            r'\(NotFound\)',  # Kubernetes NotFoundé”™è¯¯
            r'\(Forbidden\)',  # Kubernetes Forbiddené”™è¯¯
            r'\(BadRequest\)',  # Kubernetes BadRequesté”™è¯¯
            r'\(Conflict\)',  # Kubernetes Conflicté”™è¯¯
            r'\(InternalError\)',  # Kubernetes InternalError
            r'\(Unauthorized\)',  # Kubernetes Unauthorizedé”™è¯¯
            r'"error"\s*:\s*true',  # JSONä¸­çš„error: true
            r'Error parsing response',  # è§£æé”™è¯¯
            r'No API call found',  # APIè°ƒç”¨æœªæ‰¾åˆ°
            r'command not found',  # å‘½ä»¤æœªæ‰¾åˆ°
            r'permission denied',  # æƒé™æ‹’ç»
            r'no such file',  # æ–‡ä»¶ä¸å­˜åœ¨
            r'cannot access',  # æ— æ³•è®¿é—®
            r'failed to',  # å¤±è´¥
            r'unable to',  # æ— æ³•
            r'not found',  # æœªæ‰¾åˆ°
            r'does not exist',  # ä¸å­˜åœ¨
            r'already exists',  # å·²å­˜åœ¨ï¼ˆæŸäº›æƒ…å†µä¸‹æ˜¯é”™è¯¯ï¼‰
            r'connection refused',  # è¿æ¥è¢«æ‹’ç»
            r'timeout',  # è¶…æ—¶
            r'timed out',  # è¶…æ—¶
        ]

        for pattern in error_patterns:
            if re.search(pattern, result, re.IGNORECASE):
                return False

        return True

    def _prepare_input(self, task_instruction, context: Dict[str, Any], **kwargs) -> str:  # æ”¹å
        """å‡†å¤‡LLMè¾“å…¥"""
        execution_round = kwargs.get("execution_round", self.execution_round)
        successful_history = kwargs.get("successful_execution_history", self.successful_commands)
        failed_history = kwargs.get("failed_execution_history", self.failed_commands)
        probe_result = kwargs.get("probe_result", None)
        current_subtask = kwargs.get("current_subtask", None)
        executor_context = kwargs.get("executor_context", "")
        
        # åˆ¤æ–­ä½¿ç”¨å“ªç§promptæ¨¡æ¿
        use_probe_prompt = kwargs.get("use_probe_prompt", False)
        use_error_analysis_prompt = kwargs.get("use_error_analysis_prompt", False)
        use_with_error_analysis_prompt = kwargs.get("use_with_error_analysis_prompt", False)
        failed_command = kwargs.get("failed_command", "")
        error_message = kwargs.get("error_message", "")
        error_analysis_report = kwargs.get("error_analysis_report", "")
        
        # è·å–å½“å‰ iteration ç¼–å·
        current_iteration = current_subtask.iteration_number if current_subtask else 1

        # æ„å»ºprobeç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼ˆå®Œæ•´æ¢æµ‹ç»“æœï¼‰
        probe_context = ""
        if probe_result and not use_probe_prompt:  # å¸¸è§„åœºæ™¯æ˜¾ç¤ºåœ¨probe_context
            # é™åˆ¶é•¿åº¦
            max_probe_ctx_length = 8000  # çº¦2000 tokens
            if len(probe_result) > max_probe_ctx_length:
                probe_context = (
                    probe_result[:max_probe_ctx_length] + 
                    f"\n\n... [Truncated: {len(probe_result) - max_probe_ctx_length} chars omitted] ..."
                )
            else:
                probe_context = probe_result
        
        # å°†JSONæ ¼å¼çš„executor_contextè½¬æ¢ä¸ºMarkdownæ ¼å¼
        executor_context_display = self._format_executor_context_to_markdown(executor_context)
        
        # ä¸æˆªæ–­executor_contextï¼Œä¿ç•™å®Œæ•´ä¿¡æ¯ä¾›æ‰§è¡Œå™¨ä½¿ç”¨
        
        # ä»executor_contextä¸­æå–å…³é”®å­—æ®µï¼ˆç”¨äºç®€çŸ­æç¤ºï¼‰
        root_cause_component = "Unknown"
        root_cause_issue = "Unknown"
        namespace = "Unknown"
        
        if isinstance(executor_context, dict):
            root_cause = executor_context.get('root_cause', {})
            root_cause_component = root_cause.get('component', 'Unknown')
            root_cause_issue = root_cause.get('issue', 'Unknown')
            
            resources = executor_context.get('resources', {})
            namespace = resources.get('namespace', 'Unknown')

            fix_strategy = executor_context.get('fix_strategy', {})
            fix_method = fix_strategy.get('method', 'Unknown')
        
        # å‡†å¤‡å‚æ•°
        params = {
            "task_instruction": task_instruction,
            "successful_execution_history": self._format_execution_history(successful_history, success=True, current_iteration=current_iteration),
            "failed_execution_history": self._format_execution_history(failed_history, success=False, current_iteration=current_iteration),
            "probe_context": probe_context,
            "executor_context": executor_context_display,
            "execution_round": execution_round,
            "max_iterations": self.max_iterations,
            # é¢å¤–çš„å…³é”®å­—æ®µï¼ˆç”¨äºç®€çŸ­æç¤ºï¼‰
            "root_cause_component": root_cause_component,
            "root_cause_issue": root_cause_issue,
            "namespace": namespace,
            "fix_method": fix_method
        }
        
        # æ ¹æ®åœºæ™¯é€‰æ‹©ä¸åŒçš„promptæ¨¡æ¿
        if use_error_analysis_prompt:
            # é”™è¯¯åˆ†ææ¨¡å¼ï¼šåªåˆ†æé”™è¯¯
            params["failed_command"] = failed_command
            params["error_message"] = error_message
            prompt_type = "user_error_analysis"
        elif use_with_error_analysis_prompt:
            # åŸºäºé”™è¯¯åˆ†æç”Ÿæˆå‘½ä»¤æ¨¡å¼
            params["error_analysis_report"] = error_analysis_report
            prompt_type = "user_with_error_analysis"
        elif use_probe_prompt:
            # ä½¿ç”¨æ¢æµ‹åçš„ä¸“ç”¨prompt
            # é™åˆ¶probe_resulté•¿åº¦
            max_probe_length = 8000  # çº¦2000 tokens
            if probe_result and len(probe_result) > max_probe_length:
                probe_result_display = (
                    probe_result[:max_probe_length] + 
                    f"\n\n... [Truncated: {len(probe_result) - max_probe_length} chars omitted] ..."
                )
            else:
                probe_result_display = probe_result or ""
            params["probe_result"] = probe_result_display  # æ¢æµ‹ç»“æœæ”¾åœ¨ä¸“é—¨çš„ä½ç½®
            prompt_type = "user_with_probe"
        else:
            # ä½¿ç”¨æ ‡å‡†prompt
            prompt_type = "user"

        return self.prompt_loader.get_prompt(
            agent_type="executor",
            prompt_type=prompt_type,
            **params
        )

    def _format_available_actions(self) -> str:
        """æ ¼å¼åŒ–å¯ç”¨åŠ¨ä½œ"""
        if not self.available_actions:
            return "No specific actions defined - use standard kubectl commands"

        formatted = []
        for action_name, action_desc in self.available_actions.items():
            # ä¿ç•™å®Œæ•´çš„APIæ–‡æ¡£
            formatted.append(f"**{action_name}**: {action_desc}")

        return "\n\n".join(formatted)

    def _format_failure_summaries(self) -> str:
        """
        æ ¼å¼åŒ–ç´¯ç§¯çš„å¤±è´¥æ€»ç»“
        
        Returns:
            æ ¼å¼åŒ–çš„å¤±è´¥æ€»ç»“å­—ç¬¦ä¸²
        """
        if not self.failure_summaries:
            return "No previous failure summaries in this task period."
        
        formatted = []
        formatted.append(f"**Total Failures Analyzed**: {len(self.failure_summaries)}\n")
        
        for i, summary in enumerate(self.failure_summaries, 1):
            formatted.append(f"### Failure #{i} (Iter {summary['iteration']}, Round {summary['round']})")
            formatted.append(f"**Failed Command**: `{summary['failed_command']}`")
            formatted.append(f"**Error**: {summary['error']}")
            formatted.append(f"**Analysis**: {summary['analysis']}")
            formatted.append("")  # ç©ºè¡Œåˆ†éš”
        
        return "\n".join(formatted)
    
    def _format_executor_context_to_markdown(self, executor_context) -> str:
        """
        å°†JSONæ ¼å¼çš„executor_contextè½¬æ¢ä¸ºæ¸…æ™°çš„Markdownæ ¼å¼
        
        Args:
            executor_context: Observeræä¾›çš„è¯Šæ–­ä¿¡æ¯ï¼Œå¯èƒ½æ˜¯dictæˆ–str
            
        Returns:
            str: æ ¼å¼åŒ–çš„Markdownæ–‡æœ¬
        """
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºJSON
        if isinstance(executor_context, str):
            if not executor_context or executor_context.strip() == "":
                return "No diagnostic context provided"
            # å°è¯•è§£æJSON
            try:
                import json
                executor_context = json.loads(executor_context)
            except:
                # å¦‚æœä¸æ˜¯JSONï¼Œç›´æ¥è¿”å›åŸæ–‡æœ¬ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
                return executor_context
        
        # å¦‚æœä¸æ˜¯dictï¼Œè¿”å›ç©º
        if not isinstance(executor_context, dict):
            return str(executor_context)
        
        md_parts = []
        
        # 1. é—®é¢˜ç±»å‹
        problem_type = executor_context.get('problem_type', 'Unknown')
        md_parts.append(f"### ğŸ” Problem Type\n**{problem_type}**\n")
        
        # 2. æ ¹å› åˆ†æ
        root_cause = executor_context.get('root_cause', {})
        if root_cause:
            md_parts.append("### ğŸ¯ Root Cause (THIS IS WHAT YOU NEED TO FIX)\n")
            md_parts.append(f"- **Component**: `{root_cause.get('component', 'Unknown')}`")
            md_parts.append(f"- **Issue**: {root_cause.get('issue', 'Unknown')}")
            
            evidence = root_cause.get('evidence', [])
            if evidence:
                md_parts.append("\n**Evidence (Verbatim Errors)**:")
                for i, err in enumerate(evidence, 1):
                    md_parts.append(f"{i}. `{err}`")
            md_parts.append("")
        
        # 3. ç—‡çŠ¶ï¼ˆä¸è¦ä¿®å¤è¿™äº›ï¼‰
        symptoms = executor_context.get('symptoms', [])
        if symptoms:
            md_parts.append("### ğŸ“Š Symptoms (Effects - Don't Fix These)\n")
            for symptom in symptoms:
                comp = symptom.get('component', 'Unknown')
                status = symptom.get('status', 'Unknown')
                desc = symptom.get('description', '')
                md_parts.append(f"- **{comp}**: `{status}`")
                if desc:
                    md_parts.append(f"  - {desc}")
            md_parts.append("")
        
        # 4. èµ„æºä¿¡æ¯
        resources = executor_context.get('resources', {})
        if resources:
            md_parts.append("### ğŸ·ï¸ Resources (Use Exact Names)\n")
            
            namespace = resources.get('namespace')
            if namespace:
                md_parts.append(f"- **Namespace**: `{namespace}` âš ï¸ REQUIRED")
            
            for key in ['affected_services', 'affected_pods', 'affected_deployments']:
                values = resources.get(key, [])
                if values:
                    label = key.replace('_', ' ').replace('affected ', '').title()
                    items = ', '.join([f'`{v}`' for v in values])
                    md_parts.append(f"- **{label}**: {items}")
            
            config_details = resources.get('config_details', {})
            if config_details:
                md_parts.append("\n**Configuration**:")
                for k, v in config_details.items():
                    md_parts.append(f"- {k}: {v}")
            md_parts.append("")

        # 5. ä¿®å¤ç­–ç•¥ï¼ˆå¦‚æœæœ‰ï¼‰
        fix_strategy = executor_context.get('fix_strategy', {})
        if fix_strategy:
            md_parts.append("### ğŸ”§ Fix Strategy\n")

            method = fix_strategy.get('method', '')
            if method:
                md_parts.append(f"**Method**: {method}")
            
            priority = fix_strategy.get('priority', '')
            if priority:
                md_parts.append(f"**Priority**: {priority}")

            commands = fix_strategy.get('commands', [])
            if commands:
                md_parts.append("\n**Commands**:")
                for i, cmd in enumerate(commands, 1):
                    md_parts.append(f"{i}. `{cmd}`")

            verification_steps = fix_strategy.get('verification_steps', [])
            if verification_steps:
                md_parts.append("\n**Verification Steps**:")
                for i, step in enumerate(verification_steps, 1):
                    md_parts.append(f"{i}. {step}")

            fallback_plan = fix_strategy.get('fallback_plan', '')
            if fallback_plan:
                md_parts.append(f"\n**Fallback Plan**: {fallback_plan}")
            
            md_parts.append("")
        
        return "\n".join(md_parts)

    def _format_execution_history(self, execution_history, success: bool = True, current_iteration: int = None) -> str:
        """
        æ ¼å¼åŒ–æ‰§è¡Œå†å²
        - æˆåŠŸå‘½ä»¤ï¼šæ€»æ˜¯æ˜¾ç¤ºå‘½ä»¤+å®Œæ•´ç»“æœï¼ˆæ‰§è¡Œç»“æœé€šå¸¸ç®€çŸ­ï¼‰
        - å¤±è´¥å‘½ä»¤ï¼šåªæ˜¾ç¤ºæœ€åä¸€ä¸ªçš„å‘½ä»¤+é”™è¯¯ï¼Œå…¶ä»–åªæ˜¾ç¤ºå‘½ä»¤
        """
        if not execution_history:
            return f"No {'successful' if success else 'failed'} commands yet."

        history_type = "âœ“ SUCCESSFUL" if success else "âœ— FAILED"

        # æ ¹æ®æˆåŠŸ/å¤±è´¥æ˜¾ç¤ºä¸åŒæ•°é‡
        display_count = 10 if success else 5

        # è·å–æœ€è¿‘çš„å‘½ä»¤
        recent_items = execution_history[-display_count:]
        
        formatted_commands = []
        
        for i, item in enumerate(recent_items):
            if isinstance(item, dict):
                command = item.get("command", "")
                result = item.get("result", "")
                item_iteration = item.get("iteration", 0)
                item_round = item.get("round", 0)
            else:
                # å…¼å®¹æ—§æ ¼å¼ï¼ˆå­—ç¬¦ä¸²ï¼‰
                command = item
                result = ""
                item_iteration = 0
                item_round = 0
            
            if success:
                # æˆåŠŸå‘½ä»¤ï¼šæ€»æ˜¯æ˜¾ç¤ºå®Œæ•´ç»“æœï¼ˆæ‰§è¡Œç»“æœé€šå¸¸ç®€çŸ­ï¼‰
                if result:
                    # æ‰§è¡Œç»“æœé€šå¸¸ç®€çŸ­ï¼Œä¸æˆªæ–­
                    formatted_commands.append(f"  - Iter {item_iteration}, Round {item_round}: {command}\n    âœ… Result: {result}")
                else:
                    formatted_commands.append(f"  - {command}")
            else:
                # å¤±è´¥å‘½ä»¤ï¼šåªæœ‰æœ€åä¸€ä¸ªæ˜¾ç¤ºé”™è¯¯ï¼Œå…¶ä»–åªæ˜¾ç¤ºå‘½ä»¤
                if i == len(recent_items) - 1 and result:
                    error_preview = result[:300] + "..." if len(result) > 300 else result
                    formatted_commands.append(f"  - {command}\n    âŒ Error: {error_preview}")
                else:
                    formatted_commands.append(f"  - {command}")
        
        # å¦‚æœå†å²è¶…è¿‡æ˜¾ç¤ºæ•°é‡ï¼Œæ·»åŠ æç¤º
        header = ""
        if len(execution_history) > display_count:
            header = f"[{history_type}] Showing last {display_count} of {len(execution_history)} commands:\n"
        else:
            header = f"[{history_type}] {len(execution_history)} commands:\n"
        
        return header + "\n".join(formatted_commands)

    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """è§£æLLMå“åº”"""
        parsed = {
            "raw_response": response,
            "analysis": "",
            "use_probe": False,
            "probe_instruction": "",
            "executor_command": None,
            "next_action": "CONTINUE",
            # é”™è¯¯åˆ†ææ¨¡å¼å­—æ®µ
            "error_category": "",
            "root_cause": "",
            "improvement_direction": ""
        }

        try:
            # å°è¯•JSONè§£æ
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_data = json.loads(json_match.group())

                # æå–æ‰€æœ‰å­—æ®µ
                for key in ["analysis", "use_probe", "probe_instruction",
                            "executor_command", "next_action",
                            "error_category", "root_cause", "improvement_direction"]:
                    if key in json_data:
                        parsed[key] = json_data[key]

        except Exception as e:
            self.logger.error(f"Failed to parse LLM response: {e}")

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å‘½ä»¤ï¼Œå°è¯•å…¶ä»–æ–¹å¼æå–
        if not parsed["executor_command"] and not parsed["use_probe"] and not parsed["error_category"]:
            command_patterns = [
                r'"executor_command"\s*:\s*"([^"]+)"',
                r'`([^`]+)`',  # ä»£ç å—ä¸­çš„å‘½ä»¤
                r'kubectl\s+[^\n]+',  # kubectlå‘½ä»¤
                r'exec_shell\(["\']([^"\']+)["\']\)',  # exec_shellæ ¼å¼
            ]

            for pattern in command_patterns:
                match = re.search(pattern, response)
                if match:
                    parsed["executor_command"] = match.group(1) if '(' in pattern else match.group(0)
                    break

        return parsed

    def _process_decision(self,
                          decision: Dict[str, Any],
                          context: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å†³ç­–ç»“æœ"""
        self.execution_round += 1

        # æå–å†³ç­–å†…å®¹
        analysis = decision.get("analysis", "")
        use_probe = decision.get("use_probe", False)
        executor_command = decision.get("executor_command", "")
        next_action = decision.get("next_action", "CONTINUE")
        
        # é”™è¯¯åˆ†ææ¨¡å¼å­—æ®µ
        error_category = decision.get("error_category", "")
        root_cause = decision.get("root_cause", "")
        improvement_direction = decision.get("improvement_direction", "")

        # æ˜¾ç¤ºåˆ†æ
        if analysis:
            self.agent_logger.info(f"ğŸ“ Analysis: {analysis[:200]}...")

        # å¦‚æœæ˜¯é”™è¯¯åˆ†ææ¨¡å¼
        if error_category:
            self.agent_logger.info(f"ğŸ” Error Analysis Mode")
            self.agent_logger.info(f"  Category: {error_category}")
            self.agent_logger.info(f"  Root Cause: {root_cause[:150]}...")
            self.agent_logger.info(f"  Improvement: {improvement_direction[:150]}...")
            
            return {
                "round": self.execution_round,
                "mode": "error_analysis",
                "error_category": error_category,
                "root_cause": root_cause,
                "improvement_direction": improvement_direction,
                "next_action": next_action,
                "timestamp": datetime.now().isoformat()
            }

        if not use_probe and not executor_command:
            self.agent_logger.error("âŒ No executor command found")
            return {
                "round": self.execution_round,
                "use_probe": False,
                "executor_command": "",
                "next_action": "COMPLETE",
                "error": "No executor command found",
                "timestamp": datetime.now().isoformat()
            }

        # è¾“å‡ºæ‰§è¡Œä¿¡æ¯
        if use_probe:
            self.agent_logger.info(f"ğŸ” Round {self.execution_round}: Requesting probe first")
            self.agent_logger.info(f"ğŸ“‹ Probe instruction: {decision.get('probe_instruction', '')[:100]}...")
        else:
            self.agent_logger.info(f"âš¡ Round {self.execution_round}: {executor_command[:80]}...")

        # æ„å»ºç»“æœ
        result = {
            "round": self.execution_round,
            "analysis": analysis,
            "use_probe": use_probe,
            "probe_instruction": decision.get("probe_instruction", "") if use_probe else "",
            "executor_command": executor_command if not use_probe else "",
            "next_action": next_action,
            "timestamp": datetime.now().isoformat()
        }

        self.execution_results.append(result)
        return result

    def _get_fallback_decision(self) -> Dict[str, Any]:
        """è·å–å¤‡ç”¨å†³ç­–"""
        return {
            "use_probe": False,
            "executor_command": "",
            "next_action": "COMPLETE",
            "error": "Failed to get valid decision from LLM",
            "round": self.execution_round
        }

    async def executor_system(self,
                              task_instruction,
                              current_subtask: Optional[SubTaskItem] = None,
                              successful_execution_history: List[Dict] = None,
                              failed_execution_history: List[Dict] = None,
                              execution_round: int = None,
                              probe_result: Any = None,
                              executor_context: str = "",
                              use_probe_prompt: bool = False,
                              use_error_analysis_prompt: bool = False,
                              use_with_error_analysis_prompt: bool = False,
                              failed_command: str = "",
                              error_message: str = "",
                              error_analysis_report: str = "") -> Dict[str, Any]:
        """
        æ‰§è¡Œç³»ç»Ÿä¿®å¤ - ä¸»è¦æ¥å£

        Args:
            task_instruction: å­ä»»åŠ¡æŒ‡ç¤ºæ¥è‡ªè§‚å¯Ÿå™¨
            current_subtask: å½“å‰å­ä»»åŠ¡
            successful_execution_history: æˆåŠŸçš„æ‰§è¡Œå†å²
            failed_execution_history: å¤±è´¥çš„æ‰§è¡Œå†å²
            execution_round: å½“å‰æ‰§è¡Œè½®æ¬¡
            probe_result: æ¢æµ‹ç»“æœ
            executor_context: è§‚å¯Ÿå™¨æä¾›çš„æ‰§è¡Œå…³é”®ä¸Šä¸‹æ–‡
            use_probe_prompt: æ˜¯å¦ä½¿ç”¨æ¢æµ‹åçš„prompt
            use_error_analysis_prompt: æ˜¯å¦ä½¿ç”¨é”™è¯¯åˆ†ææ¨¡å¼
            use_with_error_analysis_prompt: æ˜¯å¦ä½¿ç”¨åŸºäºé”™è¯¯åˆ†æçš„å‘½ä»¤ç”Ÿæˆæ¨¡å¼
            failed_command: å¤±è´¥çš„å‘½ä»¤
            error_message: é”™è¯¯ä¿¡æ¯
            error_analysis_report: é”™è¯¯åˆ†ææŠ¥å‘Š
        Returns:
            åŒ…å«å†³ç­–ä¿¡æ¯çš„å­—å…¸
        """
        try:
            if execution_round is not None:
                self.execution_round = execution_round - 1  # å› ä¸º_process_decisionä¼š+1

            result = await self.process(
                task_instruction,
                context={},
                current_subtask=current_subtask,
                execution_round=execution_round or (self.execution_round + 1),
                successful_execution_history=successful_execution_history or [],
                failed_execution_history=failed_execution_history or [],
                probe_result=probe_result,
                executor_context=executor_context,
                use_probe_prompt=use_probe_prompt,
                use_error_analysis_prompt=use_error_analysis_prompt,
                use_with_error_analysis_prompt=use_with_error_analysis_prompt,
                failed_command=failed_command,
                error_message=error_message,
                error_analysis_report=error_analysis_report
            )

            if isinstance(result, dict):
                return result
            else:
                self.logger.error(f"Unexpected result type: {type(result)}")
                return self._get_fallback_decision()

        except Exception as e:
            self.logger.error(f"Error in executor_system: {e}")
            return self._get_fallback_decision()

    async def executor_run(self,
                           task_instruction,
                           execute_action: Callable,
                           current_subtask: Optional[SubTaskItem] = None,
                           session_id: Optional[str] = None,
                           executor_context: str = "") -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´æ‰§è¡Œæµç¨‹ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰

        Args:
            task_instruction: å­ä»»åŠ¡æŒ‡ç¤ºæ¥è‡ªè§‚å¯Ÿå™¨
            execute_action: æ‰§è¡Œå‘½ä»¤çš„å‡½æ•°
            current_subtask: å½“å‰å­ä»»åŠ¡
            session_id: ä¼šè¯ID
            executor_context: æ‰§è¡Œå™¨ä¸Šä¸‹æ–‡ï¼ˆæ¥è‡ªObserverï¼‰

        Returns:
            æ‰§è¡Œç»“æœæ±‡æ€»
        """
        # é‡ç½®çŠ¶æ€
        self.reset()
        if session_id:
            self.session_id = session_id
        
        # ä» current_subtask è·å– iteration
        current_iteration = current_subtask.iteration_number if current_subtask else 1
        
        # ä»memoryåŠ è½½baseline_contextï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
        if not self.baseline_context and self.memory_manager and session_id:
            baseline_items = self.memory_manager.query_items(
                agent_type=AgentType.PROBE,  # baselineæ˜¯ç”±probeåˆ›å»ºçš„
                memory_type=MemoryType.BASELINE_CONTEXT,
                filters={"session_id": lambda x: x == session_id if isinstance(x, str) else x.get("session_id") == session_id},
                limit=1,
                sort_by="created_at",
                descending=True
            )
            if baseline_items and isinstance(baseline_items[0], BaselineContextItem):
                self.baseline_context = baseline_items[0].baseline_content
                self.agent_logger.info(f"ğŸ’¾ Loaded baseline_context from memory (iterations {baseline_items[0].iteration_numbers})")

        # åˆå§‹åŒ–
        all_results = []
        probe_result = None
        retry_count = 0  # å½“å‰è½®æ¬¡çš„é‡è¯•æ¬¡æ•°
        max_retries_per_round = 3  # æ¯è½®æœ€å¤šé‡è¯•æ¬¡æ•°

        self.agent_logger.info(f"ğŸš€ Starting executor (max {self.max_iterations} rounds)")

        round_num = 1
        while round_num <= self.max_iterations:
            try:
                # è·å–æ‰§è¡Œå†³ç­–
                decision = await self.executor_system(
                    task_instruction=task_instruction,
                    current_subtask=current_subtask,
                    successful_execution_history=self.successful_commands,
                    failed_execution_history=self.failed_commands,
                    execution_round=round_num,
                    probe_result=probe_result,
                    executor_context=executor_context
                )

                round_data = {
                    "round": round_num,
                    "timestamp": datetime.now().isoformat(),
                    "next_action": decision.get("next_action", "CONTINUE")
                }

                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢æµ‹
                if decision.get("use_probe", False) and self.use_probe and self.probe_agent:
                    # æ‰§è¡Œæ¢æµ‹ - Executorè°ƒç”¨æ—¶åªæ‰§è¡Œ1æ¬¡å¿«é€Ÿæ¢æµ‹
                    probe_instruction = decision.get("probe_instruction", "Investigate system state")
                    self.agent_logger.info(f"ğŸ” Calling probe agent (single round)...")

                    # åˆ›å»ºä¸´æ—¶çš„å•æ¬¡æ¢æµ‹å™¨ï¼ˆmax_iterations=1ï¼‰
                    from agents.probe_agent import ProbeAgent
                    single_probe = ProbeAgent(
                        llm_config=self.llm_config,
                        memory_manager=self.memory_manager,
                        max_iterations=1,  # å…³é”®ï¼šExecutorè°ƒç”¨æ—¶åªæ‰§è¡Œ1æ¬¡
                        task_description=self.task_description,
                        available_actions=self.available_actions,
                        api_instruction=self.api_instruction
                    )
                    
                    # å…³é”®ï¼šç»§æ‰¿ä¸»probe agentçš„æ‰€æœ‰é‡è¦ä¸Šä¸‹æ–‡
                    # baseline_contextåŒ…å«å‰ä¸¤ä¸ªiterationçš„åŸºç¡€ä¿¡æ¯ï¼ˆå‘½åç©ºé—´ã€æœåŠ¡åˆ—è¡¨ç­‰ï¼‰
                    if hasattr(self.probe_agent, 'baseline_context'):
                        single_probe.baseline_context = self.probe_agent.baseline_context
                        self.agent_logger.info(f"ğŸ“‹ Inherited baseline_context ({len(self.probe_agent.baseline_context)} chars)")
                    
                    # ç»§æ‰¿å‘½ä»¤å†å²ï¼Œé¿å…é‡å¤æ‰§è¡Œ
                    if hasattr(self.probe_agent, 'successful_commands'):
                        single_probe.successful_commands = self.probe_agent.successful_commands.copy()
                    if hasattr(self.probe_agent, 'failed_commands'):
                        single_probe.failed_commands = self.probe_agent.failed_commands.copy()
                    
                    probe_res = await single_probe.probe_run(
                        task_instruction=probe_instruction,
                        execute_action=execute_action,
                        current_subtask=current_subtask,
                        session_id=session_id
                    )
                    
                    # æ„å»ºç®€æ´çš„æ¢æµ‹ç»“æœ
                    probe_results_detail = probe_res.get("results", [])
                    
                    # æ ¼å¼åŒ–æ¢æµ‹ç»“æœ
                    full_probe_result = []
                    full_probe_result.append(f"## Single Probe Result")
                    
                    if probe_results_detail:
                        result_item = probe_results_detail[0]  # åªæœ‰1ä¸ªç»“æœ
                        if result_item.get("success", False):
                            cmd = result_item.get("command", "N/A")
                            output = result_item.get("result", "")
                            full_probe_result.append(f"**Command**: {cmd}")
                            full_probe_result.append(f"**Result**:\n```\n{output}\n```")
                        else:
                            error = result_item.get("error", "Unknown error")
                            full_probe_result.append(f"**Error**: {error}")
                    
                    probe_result = "\n".join(full_probe_result)
                    
                    # è®°å½•æ—¥å¿—
                    self.agent_logger.info(
                        f"ğŸ“Š Probe completed: 1 operation, "
                        f"{'successful' if probe_results_detail and probe_results_detail[0].get('success') else 'failed'}"
                    )
                    
                    # ä¿å­˜æ¢æµ‹å™¨è¿”å›çš„successfulå’Œfailedå‘½ä»¤åˆ—è¡¨
                    probe_successful = probe_res.get("successful_commands_list", [])
                    probe_failed = probe_res.get("failed_commands_list", [])
                    
                    # åˆå¹¶åˆ°æ‰§è¡Œå™¨çš„å†å²ä¸­ï¼ˆé¿å…é‡å¤ï¼‰
                    for cmd in probe_successful:
                        if cmd not in self.successful_commands:
                            self.successful_commands.append(cmd)
                    
                    for cmd in probe_failed:
                        if cmd not in self.failed_commands:
                            self.failed_commands.append(cmd)
                    
                    # ã€å…³é”®ã€‘åŒæ­¥å›ä¸» probe_agent çš„å†å²ï¼Œç¡®ä¿è·¨ iteration çš„å‘½ä»¤å†å²ä¸€è‡´æ€§
                    if self.probe_agent:
                        for cmd in probe_successful:
                            if cmd not in self.probe_agent.successful_commands:
                                self.probe_agent.successful_commands.append(cmd)
                        
                        for cmd in probe_failed:
                            if cmd not in self.probe_agent.failed_commands:
                                self.probe_agent.failed_commands.append(cmd)
                        
                        self.agent_logger.info(
                            f"ğŸ“ Synced to probe_agent: {len(probe_successful)} successful, {len(probe_failed)} failed"
                        )
                    
                    self.agent_logger.info(
                        f"ğŸ“ Merged probe results to executor: {len(probe_successful)} successful, {len(probe_failed)} failed"
                    )

                    round_data["action_type"] = "probe"
                    round_data["command"] = probe_instruction
                    round_data["result"] = probe_result if probe_result else ""  # ä¸æˆªæ–­ç»“æœ
                    round_data["success"] = True
                    round_data["full_probe_result"] = probe_result

                    # æ·»åŠ åˆ°æˆåŠŸå†å²ï¼ˆæ¢æµ‹æœ¬èº«ï¼‰
                    probe_action_record = {
                        "command": f"[PROBE] {probe_instruction}", 
                        "result": probe_result,  # ä¸æˆªæ–­ç»“æœ
                        "iteration": current_iteration,
                        "round": round_num
                    }
                    if probe_action_record not in self.successful_commands:
                        self.successful_commands.append(probe_action_record)

                    # å…³é”®æ”¹è¿›ï¼šç«‹å³åˆ©ç”¨æ¢æµ‹ç»“æœç”Ÿæˆä¿®å¤å‘½ä»¤ï¼Œè€Œä¸æ˜¯ç»§ç»­åˆ°ä¸‹ä¸€è½®
                    self.agent_logger.info(f"ğŸ”„ Using probe results to generate repair command...")
                    
                    # ä½¿ç”¨æ¢æµ‹ç»“æœé‡æ–°ç”Ÿæˆä¿®å¤å†³ç­–ï¼ˆä½¿ç”¨ä¸“é—¨çš„promptæ¨¡æ¿ï¼‰
                    retry_decision = await self.executor_system(
                        task_instruction=task_instruction,
                        current_subtask=current_subtask,
                        successful_execution_history=self.successful_commands,
                        failed_execution_history=self.failed_commands,
                        execution_round=round_num,
                        probe_result=probe_result,  # ä¼ é€’æ¢æµ‹ç»“æœ
                        executor_context=executor_context,
                        use_probe_prompt=True  # æ ‡è®°ä½¿ç”¨æ¢æµ‹åçš„prompt
                    )
                    
                    # ä½¿ç”¨æ–°çš„å†³ç­–æ›¿ä»£åŸå†³ç­–ï¼Œæ›´æ–° decision
                    if retry_decision.get("executor_command"):
                        decision["executor_command"] = retry_decision.get("executor_command")
                        self.agent_logger.info(f"âœ¨ Generated repair command based on probe: {decision['executor_command'][:80]}...")
                        # æ‰§è¡Œè¿™ä¸ªå‘½ä»¤ï¼ˆä¸‹é¢çš„ä»£ç ä¼šå¤„ç†ï¼‰
                    else:
                        self.agent_logger.warning(f"âš ï¸ No repair command generated after probe")
                        round_num += 1
                        continue

                # æ‰§è¡Œä¿®å¤å‘½ä»¤
                executor_command = decision.get("executor_command", "")
                if executor_command:
                    # è®°å½•å¼€å§‹æ—¶é—´
                    start_time = datetime.now()
                    retry_used = False

                    # æ‰§è¡Œå‘½ä»¤
                    try:
                        exec_result = execute_action(executor_command)
                        
                        # ç«‹å³å¯¹ç»“æœè¿›è¡Œå»é‡å¤„ç†
                        if exec_result and isinstance(exec_result, str) and len(exec_result) > 1000:
                            deduplicated_result, stats = deduplicate_text(exec_result)
                            if stats["reduction_ratio"] > 0.1:  # åªæœ‰å»é‡æ•ˆæœè¶…è¿‡10%æ‰è®°å½•å’Œä½¿ç”¨
                                self.agent_logger.info(
                                    f"  ğŸ“ Deduplication: {stats['original_length']} â†’ {stats['deduplicated_length']} chars "
                                    f"(reduced {stats['reduction_ratio']:.1%})"
                                )
                                exec_result = deduplicated_result
                        
                        # æ£€æµ‹å¹¶å¤„ç†CSVæ–‡ä»¶ï¼ˆget_traces/get_metricsç­‰ï¼‰
                        if self.file_reader.should_read_files(executor_command):
                            enhanced_result, read_files = await self.file_reader.process_result(
                                command=executor_command,
                                result_text=exec_result,
                                task_instruction=task_instruction
                            )
                            if read_files:
                                exec_result = enhanced_result  # ä½¿ç”¨å¢å¼ºåçš„ç»“æœ
                        
                        is_success = self._classify_result(exec_result)

                        # å¾ªç¯é‡è¯•ï¼Œç›´åˆ°æˆåŠŸæˆ–è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                        while not is_success and retry_count < max_retries_per_round:
                            self.agent_logger.warning(
                                f"âš ï¸ Command failed, attempting retry {retry_count + 1}/{max_retries_per_round}")

                            # è®°å½•å¤±è´¥çš„å‘½ä»¤ï¼ˆåŒ…å«é”™è¯¯ä¿¡æ¯ï¼‰
                            self.failed_commands.append({
                                "command": executor_command, 
                                "result": exec_result,
                                "iteration": current_iteration,
                                "round": round_num
                            })

                            retry_count += 1
                            retry_used = True

                            # === ä¸¤é˜¶æ®µé”™è¯¯å¤„ç† ===
                            
                            # é˜¶æ®µ1ï¼šé”™è¯¯åˆ†ææ¨¡å¼ - åªåˆ†æé”™è¯¯ï¼Œä¸ç”Ÿæˆå‘½ä»¤
                            self.agent_logger.info(f"ğŸ” Stage 1: Analyzing error...")
                            
                            error_analysis_decision = await self.executor_system(
                                task_instruction=task_instruction,
                                current_subtask=current_subtask,
                                successful_execution_history=self.successful_commands,
                                failed_execution_history=self.failed_commands,
                                execution_round=round_num,
                                probe_result=probe_result,
                                executor_context=executor_context,
                                use_error_analysis_prompt=True,  # æ ‡è®°ä½¿ç”¨é”™è¯¯åˆ†ææ¨¡å¼
                                failed_command=executor_command,
                                error_message=exec_result
                            )
                            
                            # æå–é”™è¯¯åˆ†æç»“æœ
                            error_category = error_analysis_decision.get("error_category", "unknown")
                            root_cause = error_analysis_decision.get("root_cause", "")
                            improvement_direction = error_analysis_decision.get("improvement_direction", "")
                            
                            # æ„å»ºé”™è¯¯åˆ†ææŠ¥å‘Š
                            error_analysis_report = f"""
## Error Analysis Report

**Error Category**: {error_category}

**Root Cause**: 
{root_cause}

**Improvement Direction**:
{improvement_direction}
"""
                            
                            self.agent_logger.info(f"  âœ… Error analyzed: {error_category}")
                            self.agent_logger.info(f"  ğŸ“‹ Root cause: {root_cause[:100]}...")
                            
                            # é˜¶æ®µ2ï¼šåŸºäºé”™è¯¯åˆ†æç”Ÿæˆæ–°å‘½ä»¤
                            self.agent_logger.info(f"ğŸ”„ Stage 2: Generating new command based on analysis...")
                            
                            retry_decision = await self.executor_system(
                                task_instruction=task_instruction,
                                current_subtask=current_subtask,
                                successful_execution_history=self.successful_commands,
                                failed_execution_history=self.failed_commands,
                                execution_round=round_num,
                                probe_result=probe_result,
                                executor_context=executor_context,
                                use_with_error_analysis_prompt=True,  # æ ‡è®°ä½¿ç”¨åŸºäºé”™è¯¯åˆ†æçš„å‘½ä»¤ç”Ÿæˆæ¨¡å¼
                                error_analysis_report=error_analysis_report
                            )
                            
                            retry_command = retry_decision.get("executor_command", "")
                            if retry_command and retry_command != executor_command:  # ç¡®ä¿ä¸æ˜¯ç›¸åŒçš„å‘½ä»¤
                                self.agent_logger.info(f"  âœ¨ New command generated: {retry_command[:80]}...")
                                exec_result = execute_action(retry_command)
                                
                                # ç«‹å³å¯¹ç»“æœè¿›è¡Œå»é‡å¤„ç†
                                if exec_result and isinstance(exec_result, str) and len(exec_result) > 1000:
                                    deduplicated_result, stats = deduplicate_text(exec_result)
                                    if stats["reduction_ratio"] > 0.1:
                                        self.agent_logger.info(
                                            f"  ğŸ“ Deduplication: {stats['original_length']} â†’ {stats['deduplicated_length']} chars "
                                            f"(reduced {stats['reduction_ratio']:.1%})"
                                        )
                                        exec_result = deduplicated_result
                                
                                is_success = self._classify_result(exec_result)
                                executor_command = retry_command  # æ›´æ–°å‘½ä»¤è®°å½•

                                if is_success:
                                    self.agent_logger.success(f"âœ… Retry successful!")
                            else:
                                self.agent_logger.warning(f"âš ï¸ Could not generate alternative command, skipping retry")
                                break  # æ— æ³•ç”Ÿæˆæ–°å‘½ä»¤ï¼Œåœæ­¢é‡è¯•

                        # æ ¹æ®æˆåŠŸ/å¤±è´¥åˆ†ç±»å­˜å‚¨
                        execution_time = (datetime.now() - start_time).total_seconds()
                        command_record = {
                            "round": round_num,
                            "action_type": "execute",
                            "command": executor_command,
                            "result": exec_result,
                            "retry_used": retry_used
                        }

                        if is_success:
                            # è®°å½•æˆåŠŸçš„å‘½ä»¤ï¼ˆåŒ…å«ç»“æœï¼‰
                            self.successful_commands.append({
                                "command": executor_command, 
                                "result": exec_result, 
                                "iteration": current_iteration,
                                "round": round_num
                            })
                            self.agent_logger.info(f"âœ… Command successful{' (after retry)' if retry_used else ''}")

                            # åªæœ‰æˆåŠŸçš„ç»“æœæ‰ä¿å­˜åˆ°Memory
                            if self.memory_manager:
                                raw_item = RawContextItem(
                                    source_agent=self.agent_type,
                                    source_agent_id=self.session_id or "",
                                    round_number=round_num,
                                    raw_output=exec_result,
                                    command=executor_command,
                                    execution_time=execution_time,
                                    success=True,
                                    metadata={
                                        "session_id": self.session_id,
                                        "source_agent": self.agent_type.value,
                                        "iteration": current_iteration,
                                        "round_number": round_num,
                                        "command": executor_command,
                                        "result": exec_result,
                                        "retry_used": retry_used
                                    }
                                )
                                self.memory_manager.add_item(raw_item, self.agent_type)

                            # æˆåŠŸåé‡ç½®é‡è¯•è®¡æ•°
                            retry_count = 0
                        else:
                            # è®°å½•å¤±è´¥çš„å‘½ä»¤ï¼ˆåŒ…å«é”™è¯¯ä¿¡æ¯ï¼‰
                            self.failed_commands.append({
                                "command": executor_command, 
                                "result": exec_result,
                                "iteration": current_iteration,
                                "round": round_num
                            })
                            self.agent_logger.warning(f"âš ï¸ Command failed after all retries")
                            # æå–é”™è¯¯ä¿¡æ¯
                            error_match = re.search(r'Error:\s*(.+?)(?:\n|$)', exec_result)
                            if error_match:
                                self.agent_logger.error(f"Error: {error_match.group(1)}")

                        # æ˜¾ç¤ºç»“æœé¢„è§ˆ
                        result_preview = exec_result[:200] + "..." if len(exec_result) > 200 else exec_result
                        self.agent_logger.info(f"ğŸ“Š Result: {result_preview}")

                        round_data["action_type"] = "execute"
                        round_data["command"] = executor_command
                        round_data["result"] = exec_result if exec_result else ""  # ä¸æˆªæ–­ç»“æœ
                        round_data["success"] = is_success
                        round_data["retry_used"] = retry_used

                    except Exception as e:
                        self.logger.error(f"Error executing command: {e}")

                        # å°è¯•é‡è¯•
                        if retry_count < max_retries_per_round:
                            retry_count += 1
                            self.agent_logger.warning(
                                f"âš ï¸ Exception occurred, attempting retry {retry_count}/{max_retries_per_round}")

                            # è®°å½•å¤±è´¥ï¼ˆåŒ…å«é”™è¯¯ä¿¡æ¯ï¼‰
                            self.failed_commands.append({
                                "command": executor_command, 
                                "result": f"Error: {str(e)}",
                                "iteration": current_iteration,
                                "round": round_num
                            })

                            # è·³åˆ°ä¸‹ä¸€è½®ï¼Œè®©LLMç”Ÿæˆæ–°ç­–ç•¥
                            round_num += 1
                            retry_count = 0
                            continue

                        exec_result = f"Error: {str(e)}"

                        # è®°å½•å¤±è´¥ï¼ˆåŒ…å«é”™è¯¯ä¿¡æ¯ï¼‰
                        self.failed_commands.append({
                            "command": executor_command, 
                            "result": exec_result,
                            "iteration": current_iteration,
                            "round": round_num
                        })

                        round_data["action_type"] = "execute"
                        round_data["command"] = executor_command
                        round_data["result"] = exec_result  # ä¸æˆªæ–­ç»“æœ
                        round_data["success"] = False

                        self.agent_logger.error(f"âŒ Command failed: {str(e)}")

                all_results.append(round_data)

                self.agent_logger.info(
                    f"Round {round_num}: action={round_data.get('action_type')}, "
                    f"success={round_data.get('success', 'N/A')}, "
                    f"retry={round_data.get('retry_used', False)}, "
                    f"next={decision.get('next_action')}"
                )

                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                if decision.get("next_action") == "COMPLETE":
                    self.agent_logger.success(f"âœ¨ Execution completed at round {round_num}")
                    break

                # å‰è¿›åˆ°ä¸‹ä¸€è½®
                round_num += 1
                retry_count = 0  # é‡ç½®é‡è¯•è®¡æ•°

                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è½®æ¬¡
                if round_num > self.max_iterations:
                    self.agent_logger.warning(f"â±ï¸ Reached maximum iterations ({self.max_iterations})")
                    break

            except Exception as e:
                self.logger.error(f"ğŸ’¥ Error in execution round {round_num}: {e}")
                error_info = {
                    "round": round_num,
                    "error": str(e),
                    "next_action": "ERROR",
                    "timestamp": datetime.now().isoformat()
                }
                all_results.append(error_info)
                break

        # æ„å»ºå®Œæ•´çš„æ‰§è¡Œå†å²å­—ç¬¦ä¸²ï¼ˆåªåŒ…å«æˆåŠŸçš„ï¼‰
        successful_history = "\n".join([
            f"Round {item['round']} [{item.get('action_type', 'execute').upper()}]: "
            f"{item.get('command', '')}\nResult: {item.get('result', '')}"  # ä¸æˆªæ–­ç»“æœ
            for item in all_results if item.get('success', False)
        ])

        # è¿”å›ç»“æœ
        return {
            "total_rounds": len(all_results),
            "completed": True,
            "results": all_results,
            "execution_history": successful_history,
            "successful_commands": len(self.successful_commands),
            "failed_commands": len(self.failed_commands),
            "final_status": all_results[-1]["next_action"] if all_results else "NO_RESULTS",
            "session_id": self.session_id,
            "retries_used": sum(1 for r in all_results if r.get("retry_used", False))
        }

    def reset(self):
        """é‡ç½®æ‰§è¡Œå™¨çŠ¶æ€"""
        super().reset()
        self.execution_results = []
        self.execution_round = 0
        # ä¸å†é‡ç½®å‘½ä»¤å†å²ï¼Œè®©å®ƒä»¬åœ¨æ•´ä¸ªä»»åŠ¡æœŸé—´ç´¯ç§¯
        # self.successful_commands = []
        # self.failed_commands = []